{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoa92ng/Homework/blob/main/embedding_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Row Names,instance_auroc,full_pixel_auroc,anomaly_pixel_auroc\n",
        "\n",
        "mvtec_bottle,1.0,0.9848388223156217,0.9795612113551637\n",
        "\n",
        "mvtec_cable,0.9968140929535232,0.9841372038295886,0.9752826952461647\n",
        "\n",
        "mvtec_capsule,0.9792580773833267,0.9895443632504134,0.9874208674147473\n",
        "\n",
        "mvtec_carpet,0.988362760834671,0.990496871831108,0.9878582223166695\n",
        "\n",
        "mvtec_grid,0.9791144527986633,0.988007840848073,0.9836616355360858\n",
        "\n",
        "mvtec_hazelnut,1.0,0.9867676790989195,0.9791327092006923\n",
        "\n",
        "mvtec_leather,1.0,0.9928831713401822,0.9903851327309942\n",
        "\n",
        "mvtec_metal_nut,0.9990224828934506,0.9834029444579608,0.979223844081265\n",
        "\n",
        "mvtec_pill,0.9667212220403709,0.9780118319383861,0.9760501580403979\n",
        "\n",
        "mvtec_screw,0.9877023980323837,0.9952476847840384,0.9938280036555261\n",
        "\n",
        "mvtec_tile,0.9949494949494949,0.957116990925436,0.9407954758215441\n",
        "\n",
        "mvtec_toothbrush,1.0,0.9857315428686503,0.9799756240845601\n",
        "\n",
        "mvtec_transistor,0.9987499999999999,0.9609810989424673,0.9274338615727747\n",
        "\n",
        "mvtec_wood,0.9912280701754386,0.9507743250682259,0.9373950548422444\n",
        "\n",
        "mvtec_zipper,0.9950105042016807,0.9886782466534234,0.9856368251785296\n",
        "\n",
        "Mean,0.9917955704175335,0.9811080412101663,0.9735760880718238\n"
      ],
      "metadata": {
        "id": "0gAKWx_s6vSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"PatchCore and PatchCore detection methods.\"\"\"\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "\n",
        "import patchcore\n",
        "import patchcore.backbones\n",
        "import patchcore.common\n",
        "import patchcore.sampler\n",
        "from patchcore.siamese import SiameseNetwork\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PatchCore(torch.nn.Module):\n",
        "    def __init__(self, device):\n",
        "        \"\"\"PatchCore anomaly detection class.\"\"\"\n",
        "        super(PatchCore, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def load(\n",
        "        self,\n",
        "        backbone,\n",
        "        layers_to_extract_from,\n",
        "        device,\n",
        "        input_shape,\n",
        "        pretrain_embed_dimension,\n",
        "        target_embed_dimension,\n",
        "        patchsize=3,\n",
        "        patchstride=1,\n",
        "        anomaly_score_num_nn=1,\n",
        "        featuresampler=patchcore.sampler.IdentitySampler(),\n",
        "        nn_method=patchcore.common.FaissNN(False, 4),\n",
        "        sub_model=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.backbone = backbone.to(device)\n",
        "        self.layers_to_extract_from = layers_to_extract_from\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        self.device = device\n",
        "        self.patch_maker = PatchMaker(patchsize, stride=patchstride)\n",
        "\n",
        "        self.forward_modules = torch.nn.ModuleDict({})\n",
        "\n",
        "        feature_aggregator = patchcore.common.NetworkFeatureAggregator(\n",
        "            self.backbone, self.layers_to_extract_from, self.device\n",
        "        )\n",
        "        feature_dimensions = feature_aggregator.feature_dimensions(input_shape)\n",
        "        self.forward_modules[\"feature_aggregator\"] = feature_aggregator\n",
        "\n",
        "        preprocessing = patchcore.common.Preprocessing(\n",
        "            feature_dimensions, pretrain_embed_dimension\n",
        "        )\n",
        "        self.forward_modules[\"preprocessing\"] = preprocessing\n",
        "\n",
        "        self.target_embed_dimension = target_embed_dimension\n",
        "        preadapt_aggregator = patchcore.common.Aggregator(\n",
        "            target_dim=target_embed_dimension\n",
        "        )\n",
        "\n",
        "        _ = preadapt_aggregator.to(self.device)\n",
        "\n",
        "        self.forward_modules[\"preadapt_aggregator\"] = preadapt_aggregator\n",
        "\n",
        "        self.anomaly_scorer = patchcore.common.NearestNeighbourScorer(\n",
        "            n_nearest_neighbours=anomaly_score_num_nn, nn_method=nn_method\n",
        "        )\n",
        "\n",
        "        self.anomaly_segmentor = patchcore.common.RescaleSegmentor(\n",
        "            device=self.device, target_size=input_shape[-2:]\n",
        "        )\n",
        "\n",
        "        self.featuresampler = featuresampler\n",
        "\n",
        "        if sub_model:\n",
        "            self.sub_model = SiameseNetwork(in_dimension=1536, out_dimension=target_embed_dimension).to(device=device)\n",
        "            self.sub_model.load_state_dict(torch.load('./sub_model/siamese_wide_resnet50_2_final.pth'))\n",
        "\n",
        "    def embed(self, data):\n",
        "        if isinstance(data, torch.utils.data.DataLoader):\n",
        "            features = []\n",
        "            for image in data:\n",
        "                if isinstance(image, dict):\n",
        "                    image = image[\"image\"]\n",
        "                with torch.no_grad():\n",
        "                    input_image = image.to(torch.float).to(self.device)\n",
        "                    features.append(self._embed(input_image))\n",
        "            return features\n",
        "        return self._embed(data)\n",
        "\n",
        "    def _embed(self, images, detach=True, provide_patch_shapes=False):\n",
        "        \"\"\"Returns feature embeddings for images.\"\"\"\n",
        "\n",
        "        def _detach(features):\n",
        "            if detach:\n",
        "                return [x.detach().cpu().numpy() for x in features]\n",
        "            return features\n",
        "\n",
        "        _ = self.forward_modules[\"feature_aggregator\"].eval()\n",
        "        with torch.no_grad():\n",
        "            features = self.forward_modules[\"feature_aggregator\"](images)\n",
        "\n",
        "        features = [features[layer] for layer in self.layers_to_extract_from]\n",
        "\n",
        "        features = [\n",
        "            self.patch_maker.patchify(x, return_spatial_info=True) for x in features\n",
        "        ]\n",
        "        patch_shapes = [x[1] for x in features]\n",
        "        features = [x[0] for x in features]\n",
        "        ref_num_patches = patch_shapes[0]\n",
        "\n",
        "        for i in range(1, len(features)):\n",
        "            _features = features[i]\n",
        "            patch_dims = patch_shapes[i]\n",
        "\n",
        "            # TODO(pgehler): Add comments\n",
        "            _features = _features.reshape(\n",
        "                _features.shape[0], patch_dims[0], patch_dims[1], *_features.shape[2:]\n",
        "            )\n",
        "            _features = _features.permute(0, -3, -2, -1, 1, 2)\n",
        "            perm_base_shape = _features.shape\n",
        "            _features = _features.reshape(-1, *_features.shape[-2:])\n",
        "            _features = F.interpolate(\n",
        "                _features.unsqueeze(1),\n",
        "                size=(ref_num_patches[0], ref_num_patches[1]),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            _features = _features.squeeze(1)\n",
        "            _features = _features.reshape(\n",
        "                *perm_base_shape[:-2], ref_num_patches[0], ref_num_patches[1]\n",
        "            )\n",
        "            _features = _features.permute(0, -2, -1, 1, 2, 3)\n",
        "            _features = _features.reshape(len(_features), -1, *_features.shape[-3:])\n",
        "            features[i] = _features\n",
        "        features = [x.reshape(-1, *x.shape[-3:]) for x in features]\n",
        "\n",
        "        # As different feature backbones & patching provide differently\n",
        "        # sized features, these are brought into the correct form here.\n",
        "        features = self.forward_modules[\"preprocessing\"](features)\n",
        "        features = self.forward_modules[\"preadapt_aggregator\"](features)\n",
        "\n",
        "        if provide_patch_shapes:\n",
        "            return _detach(features), patch_shapes\n",
        "        return _detach(features)\n",
        "\n",
        "    def fit(self, training_data):\n",
        "        \"\"\"PatchCore training.\n",
        "\n",
        "        This function computes the embeddings of the training data and fills the\n",
        "        memory bank of SPADE.\n",
        "        \"\"\"\n",
        "        self._fill_memory_bank(training_data)\n",
        "        # self._fill_memory_bank_with_sub_model(training_data)\n",
        "\n",
        "    def get_feature(self, input_image):\n",
        "        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n",
        "\n",
        "        _ = self.forward_modules.eval()\n",
        "        with torch.no_grad():\n",
        "            input_image = input_image.to(torch.float).to(self.device)\n",
        "            _ = self.forward_modules[\"feature_aggregator\"].eval()\n",
        "            features = self.forward_modules[\"feature_aggregator\"](input_image)\n",
        "            features = [features[layer] for layer in self.layers_to_extract_from]\n",
        "            return features\n",
        "\n",
        "    def _fill_memory_bank(self, input_data):\n",
        "        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        def _image_to_features(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                return self._embed(input_image)\n",
        "\n",
        "        features = []\n",
        "        with tqdm.tqdm(\n",
        "            input_data, desc=\"Computing support features...\", position=1, leave=False\n",
        "        ) as data_iterator:\n",
        "            for image in data_iterator:\n",
        "                if isinstance(image, dict):\n",
        "                    image = image[\"image\"]\n",
        "\n",
        "                temp_feature = _image_to_features(image)\n",
        "                # temp_feature = [(x + ((i + 1) / len(temp_feature))/2) for i, x in enumerate(temp_feature)]\n",
        "                features.append(temp_feature)\n",
        "\n",
        "        features = np.concatenate(features, axis=0)\n",
        "        features = self.featuresampler.run(features)\n",
        "\n",
        "        self.anomaly_scorer.fit(detection_features=[features])\n",
        "\n",
        "    def _fill_memory_bank_with_sub_model(self, input_data, detach=True, provide_patch_shapes=False):\n",
        "        \"\"\"Computes and sets the support features for SPADE.\"\"\"\n",
        "\n",
        "        def _detach(features):\n",
        "            if detach:\n",
        "                return [x.detach().cpu().numpy() for x in features]\n",
        "            return features\n",
        "\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        def _image_to_features(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                out_feature = self.get_feature(input_image=input_image)\n",
        "                patch_shapes = [[x.shape[-2], x.shape[-1]] for x in out_feature]\n",
        "                img0, img1 = out_feature[0].to(self.device), out_feature[1].to(self.device)\n",
        "                out_feature = self.sub_model.forward_once(img0, img1)\n",
        "                out_feature = out_feature.permute(0,2,3,1)\n",
        "                out_feature = torch.reshape(out_feature, (-1, self.target_embed_dimension))\n",
        "\n",
        "                if provide_patch_shapes:\n",
        "                    return _detach(out_feature), patch_shapes\n",
        "                return _detach(out_feature)\n",
        "\n",
        "        def _image_to_features_raw(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                return self._embed(input_image)\n",
        "\n",
        "        features = []\n",
        "        with tqdm.tqdm(\n",
        "            input_data, desc=\"Computing support features...\", position=1, leave=False\n",
        "        ) as data_iterator:\n",
        "            for image in data_iterator:\n",
        "                if isinstance(image, dict):\n",
        "                    image = image[\"image\"]\n",
        "                # features.append( _image_to_features(image))\n",
        "                features.append([(x - y) * 2 for x, y in zip(_image_to_features_raw(image), _image_to_features(image))])\n",
        "\n",
        "        features = np.concatenate(features, axis=0)\n",
        "        features = self.featuresampler.run(features)\n",
        "\n",
        "        self.anomaly_scorer.fit(detection_features=[features])\n",
        "\n",
        "\n",
        "    def _embed_with_sub_model(self, input_data, detach=True, provide_patch_shapes=False):\n",
        "        def _detach(features):\n",
        "            if detach:\n",
        "                return [x.detach().cpu().numpy() for x in features]\n",
        "            return features\n",
        "        def _image_to_features(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                out_feature = self.get_feature(input_image=input_image)\n",
        "                patch_shapes = [[x.shape[-2], x.shape[-1]] for x in out_feature]\n",
        "                img0, img1 = out_feature[0].to(self.device), out_feature[1].to(self.device)\n",
        "                out_feature = self.sub_model.forward_once(img0, img1)\n",
        "                out_feature = out_feature.permute(0,2,3,1)\n",
        "                out_feature = torch.reshape(out_feature, (-1, self.target_embed_dimension))\n",
        "\n",
        "                if provide_patch_shapes:\n",
        "                    return _detach(out_feature), patch_shapes\n",
        "                return _detach(out_feature)\n",
        "\n",
        "        def _image_to_features_raw(input_image):\n",
        "            with torch.no_grad():\n",
        "                input_image = input_image.to(torch.float).to(self.device)\n",
        "                return self._embed(input_image)\n",
        "\n",
        "        return _image_to_features(input_image=input_data)\n",
        "\n",
        "    def predict(self, data):\n",
        "        if isinstance(data, torch.utils.data.DataLoader):\n",
        "            return self._predict_dataloader(data)\n",
        "        return self._predict(data)\n",
        "\n",
        "    def _predict_dataloader(self, dataloader):\n",
        "        \"\"\"This function provides anomaly scores/maps for full dataloaders.\"\"\"\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        scores = []\n",
        "        masks = []\n",
        "        labels_gt = []\n",
        "        masks_gt = []\n",
        "        with tqdm.tqdm(dataloader, desc=\"Inferring...\", leave=False) as data_iterator:\n",
        "            for image in data_iterator:\n",
        "                if isinstance(image, dict):\n",
        "                    labels_gt.extend(image[\"is_anomaly\"].numpy().tolist())\n",
        "                    masks_gt.extend(image[\"mask\"].numpy().tolist())\n",
        "                    image = image[\"image\"]\n",
        "                _scores, _masks = self._predict(image)\n",
        "                for score, mask in zip(_scores, _masks):\n",
        "                    scores.append(score)\n",
        "                    masks.append(mask)\n",
        "        return scores, masks, labels_gt, masks_gt\n",
        "\n",
        "    def _predict(self, images):\n",
        "        \"\"\"Infer score and mask for a batch of images.\"\"\"\n",
        "        images = images.to(torch.float).to(self.device)\n",
        "        _ = self.forward_modules.eval()\n",
        "\n",
        "        batchsize = images.shape[0]\n",
        "        with torch.no_grad():\n",
        "            features, patch_shapes = self._embed(images, provide_patch_shapes=True)\n",
        "            # features = [(x + ((i + 1) / len(features))/2) for i, x in enumerate(features)]\n",
        "            # features, patch_shapes = self._embed_with_sub_model(images, provide_patch_shapes=True)\n",
        "            # features = [(x - y) * 2 for x, y in zip(features_raw, features)]\n",
        "            features = np.asarray(features)\n",
        "\n",
        "            patch_scores = image_scores = self.anomaly_scorer.predict([features])[0]\n",
        "            image_scores = self.patch_maker.unpatch_scores(\n",
        "                image_scores, batchsize=batchsize\n",
        "            )\n",
        "            image_scores = image_scores.reshape(*image_scores.shape[:2], -1)\n",
        "            image_scores = self.patch_maker.score(image_scores)\n",
        "\n",
        "            patch_scores = self.patch_maker.unpatch_scores(\n",
        "                patch_scores, batchsize=batchsize\n",
        "            )\n",
        "            scales = patch_shapes[0]\n",
        "            patch_scores = patch_scores.reshape(batchsize, scales[0], scales[1])\n",
        "\n",
        "            masks = self.anomaly_segmentor.convert_to_segmentation(patch_scores)\n",
        "\n",
        "        return [score for score in image_scores], [mask for mask in masks]\n",
        "\n",
        "    @staticmethod\n",
        "    def _params_file(filepath, prepend=\"\"):\n",
        "        return os.path.join(filepath, prepend + \"patchcore_params.pkl\")\n",
        "\n",
        "    def save_to_path(self, save_path: str, prepend: str = \"\") -> None:\n",
        "        LOGGER.info(\"Saving PatchCore data.\")\n",
        "        self.anomaly_scorer.save(\n",
        "            save_path, save_features_separately=False, prepend=prepend\n",
        "        )\n",
        "        patchcore_params = {\n",
        "            \"backbone.name\": self.backbone.name,\n",
        "            \"layers_to_extract_from\": self.layers_to_extract_from,\n",
        "            \"input_shape\": self.input_shape,\n",
        "            \"pretrain_embed_dimension\": self.forward_modules[\n",
        "                \"preprocessing\"\n",
        "            ].output_dim,\n",
        "            \"target_embed_dimension\": self.forward_modules[\n",
        "                \"preadapt_aggregator\"\n",
        "            ].target_dim,\n",
        "            \"patchsize\": self.patch_maker.patchsize,\n",
        "            \"patchstride\": self.patch_maker.stride,\n",
        "            \"anomaly_scorer_num_nn\": self.anomaly_scorer.n_nearest_neighbours,\n",
        "        }\n",
        "        with open(self._params_file(save_path, prepend), \"wb\") as save_file:\n",
        "            pickle.dump(patchcore_params, save_file, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_from_path(\n",
        "        self,\n",
        "        load_path: str,\n",
        "        device: torch.device,\n",
        "        nn_method: patchcore.common.FaissNN(False, 4),\n",
        "        prepend: str = \"\",\n",
        "    ) -> None:\n",
        "        LOGGER.info(\"Loading and initializing PatchCore.\")\n",
        "        with open(self._params_file(load_path, prepend), \"rb\") as load_file:\n",
        "            patchcore_params = pickle.load(load_file)\n",
        "        patchcore_params[\"backbone\"] = patchcore.backbones.load(\n",
        "            patchcore_params[\"backbone.name\"]\n",
        "        )\n",
        "        patchcore_params[\"backbone\"].name = patchcore_params[\"backbone.name\"]\n",
        "        del patchcore_params[\"backbone.name\"]\n",
        "        self.load(**patchcore_params, device=device, nn_method=nn_method)\n",
        "\n",
        "        self.anomaly_scorer.load(load_path, prepend)\n",
        "\n",
        "\n",
        "# Image handling classes.\n",
        "class PatchMaker:\n",
        "    def __init__(self, patchsize, stride=None):\n",
        "        self.patchsize = patchsize\n",
        "        self.stride = stride\n",
        "\n",
        "    def patchify(self, features, return_spatial_info=False):\n",
        "        \"\"\"Convert a tensor into a tensor of respective patches.\n",
        "        Args:\n",
        "            x: [torch.Tensor, bs x c x w x h]\n",
        "        Returns:\n",
        "            x: [torch.Tensor, bs * w//stride * h//stride, c, patchsize,\n",
        "            patchsize]\n",
        "        \"\"\"\n",
        "        padding = int((self.patchsize - 1) / 2)\n",
        "        unfolder = torch.nn.Unfold(\n",
        "            kernel_size=self.patchsize, stride=self.stride, padding=padding, dilation=1\n",
        "        )\n",
        "        unfolded_features = unfolder(features)\n",
        "        number_of_total_patches = []\n",
        "        for s in features.shape[-2:]:\n",
        "            n_patches = (\n",
        "                s + 2 * padding - 1 * (self.patchsize - 1) - 1\n",
        "            ) / self.stride + 1\n",
        "            number_of_total_patches.append(int(n_patches))\n",
        "        unfolded_features = unfolded_features.reshape(\n",
        "            *features.shape[:2], self.patchsize, self.patchsize, -1\n",
        "        )\n",
        "        unfolded_features = unfolded_features.permute(0, 4, 1, 2, 3)\n",
        "\n",
        "        if return_spatial_info:\n",
        "            return unfolded_features, number_of_total_patches\n",
        "        return unfolded_features\n",
        "\n",
        "    def unpatch_scores(self, x, batchsize):\n",
        "        return x.reshape(batchsize, -1, *x.shape[1:])\n",
        "\n",
        "    def score(self, x):\n",
        "        was_numpy = False\n",
        "        if isinstance(x, np.ndarray):\n",
        "            was_numpy = True\n",
        "            x = torch.from_numpy(x)\n",
        "        while x.ndim > 1:\n",
        "            x = torch.max(x, dim=-1).values\n",
        "        if was_numpy:\n",
        "            return x.numpy()\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ee4uZEdY_rpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import click\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys, os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "sys.path.append('./src')\n",
        "import patchcore.backbones\n",
        "import patchcore.common\n",
        "import patchcore.metrics\n",
        "import patchcore.patchcore\n",
        "import patchcore.sampler\n",
        "import patchcore.utils\n",
        "\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n",
        "\n",
        "\n",
        "@click.group(chain=True)\n",
        "@click.argument(\"results_path\", type=str)\n",
        "@click.option(\"--gpu\", type=int, default=[0], multiple=True, show_default=True)\n",
        "@click.option(\"--seed\", type=int, default=0, show_default=True)\n",
        "@click.option(\"--log_group\", default='IM224_WR50_L2-3_P01_D1024-1024_PS-3_AN-1_S0', type=str, default=\"group\")\n",
        "@click.option(\"--log_project\", type=str, default=\"project\")\n",
        "@click.option(\"--save_segmentation_images\", is_flag=True)\n",
        "@click.option(\"--save_patchcore_model\", is_flag=True)\n",
        "def main(**kwargs):\n",
        "    pass\n",
        "\n",
        "\n",
        "@main.result_callback()\n",
        "def run(\n",
        "    methods,\n",
        "    results_path,\n",
        "    gpu,\n",
        "    seed,\n",
        "    log_group,\n",
        "    log_project,\n",
        "    save_segmentation_images,\n",
        "    save_patchcore_model,\n",
        "):\n",
        "    methods = {key: item for (key, item) in methods}\n",
        "\n",
        "    run_save_path = patchcore.utils.create_storage_folder(\n",
        "        results_path, log_project, log_group, mode=\"iterate\"\n",
        "    )\n",
        "\n",
        "    list_of_dataloaders = methods[\"get_dataloaders\"](seed)\n",
        "\n",
        "    device = patchcore.utils.set_torch_device(gpu)\n",
        "    # Device context here is specifically set and used later\n",
        "    # because there was GPU memory-bleeding which I could only fix with\n",
        "    # context managers.\n",
        "    device_context = (\n",
        "        torch.cuda.device(\"cuda:{}\".format(device.index))\n",
        "        if \"cuda\" in device.type.lower()\n",
        "        else contextlib.suppress()\n",
        "    )\n",
        "\n",
        "    result_collect = []\n",
        "\n",
        "    for dataloader_count, dataloaders in enumerate(list_of_dataloaders):\n",
        "        LOGGER.info(\n",
        "            \"Evaluating dataset [{}] ({}/{})...\".format(\n",
        "                dataloaders[\"training\"].name,\n",
        "                dataloader_count + 1,\n",
        "                len(list_of_dataloaders),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        patchcore.utils.fix_seeds(seed, device)\n",
        "\n",
        "        dataset_name = dataloaders[\"training\"].name\n",
        "\n",
        "        with device_context:\n",
        "            torch.cuda.empty_cache()\n",
        "            imagesize = dataloaders[\"training\"].dataset.imagesize\n",
        "            sampler = methods[\"get_sampler\"](\n",
        "                device,\n",
        "            )\n",
        "            PatchCore_list = methods[\"get_patchcore\"](imagesize, sampler, device)\n",
        "            if len(PatchCore_list) > 1:\n",
        "                LOGGER.info(\n",
        "                    \"Utilizing PatchCore Ensemble (N={}).\".format(len(PatchCore_list))\n",
        "                )\n",
        "            for i, PatchCore in enumerate(PatchCore_list):\n",
        "                torch.cuda.empty_cache()\n",
        "                if PatchCore.backbone.seed is not None:\n",
        "                    patchcore.utils.fix_seeds(PatchCore.backbone.seed, device)\n",
        "                LOGGER.info(\n",
        "                    \"Training models ({}/{})\".format(i + 1, len(PatchCore_list))\n",
        "                )\n",
        "                torch.cuda.empty_cache()\n",
        "                PatchCore.fit(dataloaders[\"training\"])\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            aggregator = {\"scores\": [], \"segmentations\": []}\n",
        "            for i, PatchCore in enumerate(PatchCore_list):\n",
        "                torch.cuda.empty_cache()\n",
        "                LOGGER.info(\n",
        "                    \"Embedding test data with models ({}/{})\".format(\n",
        "                        i + 1, len(PatchCore_list)\n",
        "                    )\n",
        "                )\n",
        "                scores, segmentations, labels_gt, masks_gt = PatchCore.predict(\n",
        "                    dataloaders[\"testing\"]\n",
        "                )\n",
        "                aggregator[\"scores\"].append(scores)\n",
        "                aggregator[\"segmentations\"].append(segmentations)\n",
        "\n",
        "            scores = np.array(aggregator[\"scores\"])\n",
        "            min_scores = scores.min(axis=-1).reshape(-1, 1)\n",
        "            max_scores = scores.max(axis=-1).reshape(-1, 1)\n",
        "            scores = (scores - min_scores) / (max_scores - min_scores)\n",
        "            scores = np.mean(scores, axis=0)\n",
        "\n",
        "            segmentations = np.array(aggregator[\"segmentations\"])\n",
        "            min_scores = (\n",
        "                segmentations.reshape(len(segmentations), -1)\n",
        "                .min(axis=-1)\n",
        "                .reshape(-1, 1, 1, 1)\n",
        "            )\n",
        "            max_scores = (\n",
        "                segmentations.reshape(len(segmentations), -1)\n",
        "                .max(axis=-1)\n",
        "                .reshape(-1, 1, 1, 1)\n",
        "            )\n",
        "            segmentations = (segmentations - min_scores) / (max_scores - min_scores)\n",
        "            segmentations = np.mean(segmentations, axis=0)\n",
        "\n",
        "            anomaly_labels = [\n",
        "                x[1] != \"good\" for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "            ]\n",
        "\n",
        "            # (Optional) Plot example images.\n",
        "            if save_segmentation_images:\n",
        "                image_paths = [\n",
        "                    x[2] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                ]\n",
        "                mask_paths = [\n",
        "                    x[3] for x in dataloaders[\"testing\"].dataset.data_to_iterate\n",
        "                ]\n",
        "\n",
        "                def image_transform(image):\n",
        "                    in_std = np.array(\n",
        "                        dataloaders[\"testing\"].dataset.transform_std\n",
        "                    ).reshape(-1, 1, 1)\n",
        "                    in_mean = np.array(\n",
        "                        dataloaders[\"testing\"].dataset.transform_mean\n",
        "                    ).reshape(-1, 1, 1)\n",
        "                    image = dataloaders[\"testing\"].dataset.transform_img(image)\n",
        "                    return np.clip(\n",
        "                        (image.numpy() * in_std + in_mean) * 255, 0, 255\n",
        "                    ).astype(np.uint8)\n",
        "\n",
        "                def mask_transform(mask):\n",
        "                    return dataloaders[\"testing\"].dataset.transform_mask(mask).numpy()\n",
        "\n",
        "                image_save_path = os.path.join(\n",
        "                    run_save_path, \"segmentation_images\", dataset_name\n",
        "                )\n",
        "                os.makedirs(image_save_path, exist_ok=True)\n",
        "                patchcore.utils.plot_segmentation_images(\n",
        "                    image_save_path,\n",
        "                    image_paths,\n",
        "                    segmentations,\n",
        "                    scores,\n",
        "                    mask_paths,\n",
        "                    image_transform=image_transform,\n",
        "                    mask_transform=mask_transform,\n",
        "                )\n",
        "\n",
        "            LOGGER.info(\"Computing evaluation metrics.\")\n",
        "            auroc = patchcore.metrics.compute_imagewise_retrieval_metrics(\n",
        "                scores, anomaly_labels\n",
        "            )[\"auroc\"]\n",
        "\n",
        "            # Compute PRO score & PW Auroc for all images\n",
        "            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n",
        "                segmentations, masks_gt\n",
        "            )\n",
        "            full_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "            # Compute PRO score & PW Auroc only images with anomalies\n",
        "            sel_idxs = []\n",
        "            for i in range(len(masks_gt)):\n",
        "                if np.sum(masks_gt[i]) > 0:\n",
        "                    sel_idxs.append(i)\n",
        "            pixel_scores = patchcore.metrics.compute_pixelwise_retrieval_metrics(\n",
        "                [segmentations[i] for i in sel_idxs],\n",
        "                [masks_gt[i] for i in sel_idxs],\n",
        "            )\n",
        "            anomaly_pixel_auroc = pixel_scores[\"auroc\"]\n",
        "\n",
        "            result_collect.append(\n",
        "                {\n",
        "                    \"dataset_name\": dataset_name,\n",
        "                    \"instance_auroc\": auroc,\n",
        "                    \"full_pixel_auroc\": full_pixel_auroc,\n",
        "                    \"anomaly_pixel_auroc\": anomaly_pixel_auroc,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            for key, item in result_collect[-1].items():\n",
        "                if key != \"dataset_name\":\n",
        "                    LOGGER.info(\"{0}: {1:3.3f}\".format(key, item))\n",
        "\n",
        "            # (Optional) Store PatchCore model for later re-use.\n",
        "            # SAVE all patchcores only if mean_threshold is passed?\n",
        "            if save_patchcore_model:\n",
        "                patchcore_save_path = os.path.join(\n",
        "                    run_save_path, \"models\", dataset_name\n",
        "                )\n",
        "                os.makedirs(patchcore_save_path, exist_ok=True)\n",
        "                for i, PatchCore in enumerate(PatchCore_list):\n",
        "                    prepend = (\n",
        "                        \"Ensemble-{}-{}_\".format(i + 1, len(PatchCore_list))\n",
        "                        if len(PatchCore_list) > 1\n",
        "                        else \"\"\n",
        "                    )\n",
        "                    PatchCore.save_to_path(patchcore_save_path, prepend)\n",
        "\n",
        "        LOGGER.info(\"\\n\\n-----\\n\")\n",
        "\n",
        "    # Store all results and mean scores to a csv-file.\n",
        "    result_metric_names = list(result_collect[-1].keys())[1:]\n",
        "    result_dataset_names = [results[\"dataset_name\"] for results in result_collect]\n",
        "    result_scores = [list(results.values())[1:] for results in result_collect]\n",
        "    patchcore.utils.compute_and_store_final_results(\n",
        "        run_save_path,\n",
        "        result_scores,\n",
        "        column_names=result_metric_names,\n",
        "        row_names=result_dataset_names,\n",
        "    )\n",
        "\n",
        "\n",
        "@main.command(\"patch_core\")\n",
        "# Pretraining-specific parameters.\n",
        "@click.option(\"--backbone_names\", \"-b\", type=str, multiple=True, default=[])\n",
        "@click.option(\"--layers_to_extract_from\", \"-le\", type=str, multiple=True, default=[])\n",
        "# Parameters for Glue-code (to merge different parts of the pipeline.\n",
        "@click.option(\"--pretrain_embed_dimension\", type=int, default=1024)\n",
        "@click.option(\"--target_embed_dimension\", type=int, default=1024)\n",
        "@click.option(\"--preprocessing\", type=click.Choice([\"mean\", \"conv\"]), default=\"mean\")\n",
        "@click.option(\"--aggregation\", type=click.Choice([\"mean\", \"mlp\"]), default=\"mean\")\n",
        "# Nearest-Neighbour Anomaly Scorer parameters.\n",
        "@click.option(\"--anomaly_scorer_num_nn\", type=int, default=5)\n",
        "# Patch-parameters.\n",
        "@click.option(\"--patchsize\", type=int, default=3)\n",
        "@click.option(\"--patchscore\", type=str, default=\"max\")\n",
        "@click.option(\"--patchoverlap\", type=float, default=0.0)\n",
        "@click.option(\"--patchsize_aggregate\", \"-pa\", type=int, multiple=True, default=[])\n",
        "# NN on GPU.\n",
        "@click.option(\"--faiss_on_gpu\", is_flag=True)\n",
        "@click.option(\"--faiss_num_workers\", type=int, default=8)\n",
        "def patch_core(\n",
        "    backbone_names,\n",
        "    layers_to_extract_from,\n",
        "    pretrain_embed_dimension,\n",
        "    target_embed_dimension,\n",
        "    preprocessing,\n",
        "    aggregation,\n",
        "    patchsize,\n",
        "    patchscore,\n",
        "    patchoverlap,\n",
        "    anomaly_scorer_num_nn,\n",
        "    patchsize_aggregate,\n",
        "    faiss_on_gpu,\n",
        "    faiss_num_workers,\n",
        "):\n",
        "    backbone_names = list(backbone_names)\n",
        "    if len(backbone_names) > 1:\n",
        "        layers_to_extract_from_coll = [[] for _ in range(len(backbone_names))]\n",
        "        for layer in layers_to_extract_from:\n",
        "            idx = int(layer.split(\".\")[0])\n",
        "            layer = \".\".join(layer.split(\".\")[1:])\n",
        "            layers_to_extract_from_coll[idx].append(layer)\n",
        "    else:\n",
        "        layers_to_extract_from_coll = [layers_to_extract_from]\n",
        "\n",
        "    def get_patchcore(input_shape, sampler, device):\n",
        "        loaded_patchcores = []\n",
        "        for backbone_name, layers_to_extract_from in zip(\n",
        "            backbone_names, layers_to_extract_from_coll\n",
        "        ):\n",
        "            backbone_seed = None\n",
        "            if \".seed-\" in backbone_name:\n",
        "                backbone_name, backbone_seed = backbone_name.split(\".seed-\")[0], int(\n",
        "                    backbone_name.split(\"-\")[-1]\n",
        "                )\n",
        "            backbone = patchcore.backbones.load(backbone_name)\n",
        "            backbone.name, backbone.seed = backbone_name, backbone_seed\n",
        "\n",
        "            nn_method = patchcore.common.FaissNN(faiss_on_gpu, faiss_num_workers)\n",
        "\n",
        "            patchcore_instance = patchcore.patchcore.PatchCore(device)\n",
        "            patchcore_instance.load(\n",
        "                backbone=backbone,\n",
        "                layers_to_extract_from=layers_to_extract_from,\n",
        "                device=device,\n",
        "                input_shape=input_shape,\n",
        "                pretrain_embed_dimension=pretrain_embed_dimension,\n",
        "                target_embed_dimension=target_embed_dimension,\n",
        "                patchsize=patchsize,\n",
        "                featuresampler=sampler,\n",
        "                anomaly_scorer_num_nn=anomaly_scorer_num_nn,\n",
        "                nn_method=nn_method,\n",
        "            )\n",
        "            loaded_patchcores.append(patchcore_instance)\n",
        "        return loaded_patchcores\n",
        "\n",
        "    return (\"get_patchcore\", get_patchcore)\n",
        "\n",
        "\n",
        "@main.command(\"sampler\")\n",
        "@click.argument(\"name\", type=str)\n",
        "@click.option(\"--percentage\", \"-p\", type=float, default=0.1, show_default=True)\n",
        "def sampler(name, percentage):\n",
        "    def get_sampler(device):\n",
        "        if name == \"identity\":\n",
        "            return patchcore.sampler.IdentitySampler()\n",
        "        elif name == \"greedy_coreset\":\n",
        "            return patchcore.sampler.GreedyCoresetSampler(percentage, device)\n",
        "        elif name == \"approx_greedy_coreset\":\n",
        "            return patchcore.sampler.ApproximateGreedyCoresetSampler(percentage, device)\n",
        "\n",
        "    return (\"get_sampler\", get_sampler)\n",
        "\n",
        "\n",
        "@main.command(\"dataset\")\n",
        "@click.argument(\"name\", type=str)\n",
        "@click.argument(\"data_path\", type=click.Path(exists=True, file_okay=False))\n",
        "@click.option(\"--subdatasets\", \"-d\", multiple=True, type=str, required=True)\n",
        "@click.option(\"--train_val_split\", type=float, default=1, show_default=True)\n",
        "@click.option(\"--batch_size\", default=2, type=int, show_default=True)\n",
        "@click.option(\"--num_workers\", default=8, type=int, show_default=True)\n",
        "@click.option(\"--resize\", default=256, type=int, show_default=True)\n",
        "@click.option(\"--imagesize\", default=224, type=int, show_default=True)\n",
        "@click.option(\"--augment\", is_flag=True)\n",
        "def dataset(\n",
        "    name,\n",
        "    data_path,\n",
        "    subdatasets,\n",
        "    train_val_split,\n",
        "    batch_size,\n",
        "    resize,\n",
        "    imagesize,\n",
        "    num_workers,\n",
        "    augment,\n",
        "):\n",
        "    dataset_info = _DATASETS[name]\n",
        "    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n",
        "\n",
        "    def get_dataloaders(seed):\n",
        "        dataloaders = []\n",
        "        for subdataset in subdatasets:\n",
        "            train_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                train_val_split=train_val_split,\n",
        "                imagesize=imagesize,\n",
        "                split=dataset_library.DatasetSplit.TRAIN,\n",
        "                seed=seed,\n",
        "                augment=augment,\n",
        "            )\n",
        "\n",
        "            test_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                imagesize=imagesize,\n",
        "                split=dataset_library.DatasetSplit.TEST,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "            train_dataloader = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            test_dataloader = torch.utils.data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            train_dataloader.name = name\n",
        "            if subdataset is not None:\n",
        "                train_dataloader.name += \"_\" + subdataset\n",
        "\n",
        "            if train_val_split < 1:\n",
        "                val_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                    data_path,\n",
        "                    classname=subdataset,\n",
        "                    resize=resize,\n",
        "                    train_val_split=train_val_split,\n",
        "                    imagesize=imagesize,\n",
        "                    split=dataset_library.DatasetSplit.VAL,\n",
        "                    seed=seed,\n",
        "                )\n",
        "\n",
        "                val_dataloader = torch.utils.data.DataLoader(\n",
        "                    val_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True,\n",
        "                )\n",
        "            else:\n",
        "                val_dataloader = None\n",
        "            dataloader_dict = {\n",
        "                \"training\": train_dataloader,\n",
        "                \"validation\": val_dataloader,\n",
        "                \"testing\": test_dataloader,\n",
        "            }\n",
        "\n",
        "            dataloaders.append(dataloader_dict)\n",
        "        return dataloaders\n",
        "\n",
        "    return (\"get_dataloaders\", get_dataloaders)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    LOGGER.info(\"Command line arguments: {}\".format(\" \".join(sys.argv)))\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "z65ZR7T9_vah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import sys, os\n",
        "# print(os.getcwd())\n",
        "sys.path.append('./src')\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import patchcore\n",
        "from patchcore import patchcore as patchcore_model\n",
        "\n",
        "from siamese import SiameseNetwork\n",
        "\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "\n",
        "_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n",
        "_sub_dataset = ('bottle',  'cable',  'capsule',  'carpet',  'grid', 'hazelnut', 'leather',  'metal_nut',  'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper')\n",
        "_data_path = r'D:\\dataset\\mvtec_anomaly_detection'\n",
        "\n",
        "def _standard_patchcore(image_dimension):\n",
        "    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n",
        "    backbone = models.wide_resnet50_2(pretrained=False)\n",
        "    backbone.name, backbone.seed = \"wideresnet50\", 0\n",
        "    patchcore_instance.load(\n",
        "        backbone=backbone,\n",
        "        layers_to_extract_from=[\"layer2\", \"layer3\"],\n",
        "        device=torch.device(\"cpu\"),\n",
        "        input_shape=[3, image_dimension, image_dimension],\n",
        "        pretrain_embed_dimension=1024,\n",
        "        target_embed_dimension=1024,\n",
        "        patchsize=3,\n",
        "        patchstride=1,\n",
        "        spade_nn=2,\n",
        "        sub_model=None\n",
        "    )\n",
        "    return patchcore_instance\n",
        "\n",
        "def _dummy_constant_dataloader(number_of_examples, shape_of_examples):\n",
        "    features = _dummy_features(number_of_examples, shape_of_examples)\n",
        "    return torch.utils.data.DataLoader(features, batch_size=1)\n",
        "\n",
        "\n",
        "def _dummy_features(number_of_examples, shape_of_examples):\n",
        "    return torch.Tensor(\n",
        "        np.stack(number_of_examples * [np.ones(shape_of_examples)], axis=0)\n",
        "    )\n",
        "\n",
        "def dataset(\n",
        "    name=\"mvtec\",\n",
        "    data_path=_data_path,\n",
        "    subdatasets=_sub_dataset,\n",
        "    train_val_split=1,\n",
        "    batch_size=2,\n",
        "    resize=256,\n",
        "    imagesize=224,\n",
        "    num_workers=8,\n",
        "    augment=False,\n",
        "):\n",
        "    dataset_info = _DATASETS[name]\n",
        "    dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n",
        "\n",
        "    def get_dataloaders(seed=0):\n",
        "        dataloaders = []\n",
        "        for subdataset in subdatasets:\n",
        "            train_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                train_val_split=train_val_split,\n",
        "                imagesize=imagesize,\n",
        "                split=dataset_library.DatasetSplit.TRAIN,\n",
        "                seed=seed,\n",
        "                augment=augment,\n",
        "            )\n",
        "\n",
        "            test_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                data_path,\n",
        "                classname=subdataset,\n",
        "                resize=resize,\n",
        "                imagesize=imagesize,\n",
        "                split=dataset_library.DatasetSplit.TEST,\n",
        "                seed=seed,\n",
        "            )\n",
        "\n",
        "            train_dataloader = torch.utils.data.DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            test_dataloader = torch.utils.data.DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=num_workers,\n",
        "                pin_memory=True,\n",
        "            )\n",
        "\n",
        "            train_dataloader.name = name\n",
        "            if subdataset is not None:\n",
        "                train_dataloader.name += \"_\" + subdataset\n",
        "\n",
        "            if train_val_split < 1:\n",
        "                val_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                    data_path,\n",
        "                    classname=subdataset,\n",
        "                    resize=resize,\n",
        "                    train_val_split=train_val_split,\n",
        "                    imagesize=imagesize,\n",
        "                    split=dataset_library.DatasetSplit.VAL,\n",
        "                    seed=seed,\n",
        "                )\n",
        "\n",
        "                val_dataloader = torch.utils.data.DataLoader(\n",
        "                    val_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True,\n",
        "                )\n",
        "            else:\n",
        "                val_dataloader = None\n",
        "            dataloader_dict = {\n",
        "                \"training\": train_dataloader,\n",
        "                \"validation\": val_dataloader,\n",
        "                \"testing\": test_dataloader,\n",
        "            }\n",
        "\n",
        "            dataloaders.append(dataloader_dict)\n",
        "        return dataloaders\n",
        "\n",
        "    return (\"get_dataloaders\", get_dataloaders)\n",
        "\n",
        "def test_dummy_patchcore(image_path):\n",
        "    image_dimension = 112\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "    training_dataloader = _dummy_constant_dataloader(\n",
        "        4, [3, image_dimension, image_dimension]\n",
        "    )\n",
        "    print(model.featuresampler)\n",
        "    model.fit(training_dataloader)\n",
        "\n",
        "    # test_features = torch.Tensor(2 * np.ones([2, 3, image_dimension, image_dimension]))\n",
        "    # scores, masks = model.predict(test_features)\n",
        "\n",
        "    # assert all([score > 0 for score in scores])\n",
        "    # for mask in masks:\n",
        "    #     assert np.all(mask.shape == (image_dimension, image_dimension))\n",
        "\n",
        "def reset_params(model):\n",
        "    for layer in model.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    epoch_num = 1\n",
        "    network = SiameseNetwork(in_dimension=1536).to(device='cuda')\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
        "    _, x = dataset(num_workers=1, train_val_split=1)\n",
        "    _dataset = x(0)\n",
        "    image_dimension = 224\n",
        "    model = _standard_patchcore(image_dimension)\n",
        "\n",
        "    dt_training = ConcatDataset([x['training'].dataset for x in _dataset])\n",
        "    dtloader_training = DataLoader(dt_training, batch_size=2, shuffle=True)\n",
        "\n",
        "    for epoch in range(epoch_num):\n",
        "        print(f\"=========== epoch {epoch} ===========\")\n",
        "        loss_item = 0\n",
        "        network.train()\n",
        "        for i, image in enumerate(dtloader_training):\n",
        "            feature = model.get_feature(image[\"image\"])\n",
        "            img0, img1 = feature[0], feature[1]\n",
        "            img0, img1 = img0.cuda(), img1.cuda()\n",
        "            if (img0.shape[0] < 2 or  img1.shape[0] < 2 ): continue\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output1, output2 = network(img0, img1)\n",
        "            if image['classname'][0] == image['classname'][1]:\n",
        "                loss = criterion(output1, output2)\n",
        "            else:\n",
        "                loss = 1 / criterion(output1, output2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_item += loss.item()\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epoch_num}], Step [{i}/{len(dtloader_training)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save(network.state_dict(), f'./sub_model/siamese_wide_resnet50_2_final.pth')\n",
        "        # # valid\n",
        "        # for id, data in enumerate(_dataset):\n",
        "        #     if id == 0:\n",
        "        #         for i, image in enumerate(data['validation']):\n",
        "        #             if (image[0].shape[0] < 2): continue\n",
        "        #             feature = model.get_feature(image[\"image\"])\n",
        "        #             img0, img1 = feature[0], feature[1]\n",
        "        #             img0, img1 = img0.cuda(), img1.cuda()\n",
        "\n",
        "        #             optimizer.zero_grad()\n",
        "        #             output1, output2 = network(img0, img1)\n",
        "        #             loss = criterion(output1, output2)\n",
        "        #             loss.backward()\n",
        "        #             optimizer.step()\n",
        "        #             if i % 2 == 0:\n",
        "        #                 print(f\"Epoch [{epoch+1}/{2}], Step [{i}/{len(data['training'])}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "JHvOQCp1_0gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PYTORCH-VAE-MASTER\n",
        "\n",
        "import os\n",
        "from enum import Enum\n",
        "\n",
        "import PIL\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "_CLASSNAMES = [\n",
        "    \"bottle\",\n",
        "    \"cable\",\n",
        "    \"capsule\",\n",
        "    \"carpet\",\n",
        "    \"grid\",\n",
        "    \"hazelnut\",\n",
        "    \"leather\",\n",
        "    \"metal_nut\",\n",
        "    \"pill\",\n",
        "    \"screw\",\n",
        "    \"tile\",\n",
        "    \"toothbrush\",\n",
        "    \"transistor\",\n",
        "    \"wood\",\n",
        "    \"zipper\",\n",
        "]\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "class DatasetSplit(Enum):\n",
        "    TRAIN = \"train\"\n",
        "    VAL = \"val\"\n",
        "    TEST = \"test\"\n",
        "\n",
        "\n",
        "class MVTecDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for MVTec.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        source,\n",
        "        classname,\n",
        "        resize=256,\n",
        "        imagesize=224,\n",
        "        split=DatasetSplit.TRAIN,\n",
        "        train_val_split=1.0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            source: [str]. Path to the MVTec data folder.\n",
        "            classname: [str or None]. Name of MVTec class that should be\n",
        "                       provided in this dataset. If None, the datasets\n",
        "                       iterates over all available images.\n",
        "            resize: [int]. (Square) Size the loaded image initially gets\n",
        "                    resized to.\n",
        "            imagesize: [int]. (Square) Size the resized loaded image gets\n",
        "                       (center-)cropped to.\n",
        "            split: [enum-option]. Indicates if training or test split of the\n",
        "                   data should be used. Has to be an option taken from\n",
        "                   DatasetSplit, e.g. mvtec.DatasetSplit.TRAIN. Note that\n",
        "                   mvtec.DatasetSplit.TEST will also load mask data.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.source = source\n",
        "        self.split = split\n",
        "        self.classnames_to_use = [classname] if classname is not None else _CLASSNAMES\n",
        "        self.train_val_split = train_val_split\n",
        "        self.transform_std = IMAGENET_STD\n",
        "        self.transform_mean = IMAGENET_MEAN\n",
        "        self.imgpaths_per_class, self.data_to_iterate = self.get_image_data()\n",
        "\n",
        "        self.transform_img = [\n",
        "            transforms.Resize(resize),\n",
        "            transforms.CenterCrop(imagesize),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "        ]\n",
        "        self.transform_img = transforms.Compose(self.transform_img)\n",
        "\n",
        "        self.transform_mask = [\n",
        "            transforms.Resize(resize),\n",
        "            transforms.CenterCrop(imagesize),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "        self.transform_mask = transforms.Compose(self.transform_mask)\n",
        "\n",
        "        self.imagesize = (3, imagesize, imagesize)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        classname, anomaly, image_path, mask_path = self.data_to_iterate[idx]\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform_img(image)\n",
        "\n",
        "        if self.split == DatasetSplit.TEST and mask_path is not None:\n",
        "            mask = PIL.Image.open(mask_path)\n",
        "            mask = self.transform_mask(mask)\n",
        "        else:\n",
        "            mask = torch.zeros([1, *image.size()[1:]])\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"mask\": mask,\n",
        "            \"classname\": classname,\n",
        "            \"anomaly\": anomaly,\n",
        "            \"is_anomaly\": int(anomaly != \"good\"),\n",
        "            \"image_name\": \"/\".join(image_path.split(\"/\")[-4:]),\n",
        "            \"image_path\": image_path,\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_to_iterate)\n",
        "\n",
        "    def get_image_data(self):\n",
        "        imgpaths_per_class = {}\n",
        "        maskpaths_per_class = {}\n",
        "\n",
        "        for classname in self.classnames_to_use:\n",
        "            classpath = os.path.join(self.source, classname, self.split.value)\n",
        "            maskpath = os.path.join(self.source, classname, \"ground_truth\")\n",
        "            anomaly_types = os.listdir(classpath)\n",
        "\n",
        "            imgpaths_per_class[classname] = {}\n",
        "            maskpaths_per_class[classname] = {}\n",
        "\n",
        "            for anomaly in anomaly_types:\n",
        "                anomaly_path = os.path.join(classpath, anomaly)\n",
        "                anomaly_files = sorted(os.listdir(anomaly_path))\n",
        "                imgpaths_per_class[classname][anomaly] = [\n",
        "                    os.path.join(anomaly_path, x) for x in anomaly_files\n",
        "                ]\n",
        "\n",
        "                if self.train_val_split < 1.0:\n",
        "                    n_images = len(imgpaths_per_class[classname][anomaly])\n",
        "                    train_val_split_idx = int(n_images * self.train_val_split)\n",
        "                    if self.split == DatasetSplit.TRAIN:\n",
        "                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n",
        "                            classname\n",
        "                        ][anomaly][:train_val_split_idx]\n",
        "                    elif self.split == DatasetSplit.VAL:\n",
        "                        imgpaths_per_class[classname][anomaly] = imgpaths_per_class[\n",
        "                            classname\n",
        "                        ][anomaly][train_val_split_idx:]\n",
        "\n",
        "                if self.split == DatasetSplit.TEST and anomaly != \"good\":\n",
        "                    anomaly_mask_path = os.path.join(maskpath, anomaly)\n",
        "                    anomaly_mask_files = sorted(os.listdir(anomaly_mask_path))\n",
        "                    maskpaths_per_class[classname][anomaly] = [\n",
        "                        os.path.join(anomaly_mask_path, x) for x in anomaly_mask_files\n",
        "                    ]\n",
        "                else:\n",
        "                    maskpaths_per_class[classname][\"good\"] = None\n",
        "\n",
        "        # Unrolls the data dictionary to an easy-to-iterate list.\n",
        "        data_to_iterate = []\n",
        "        for classname in sorted(imgpaths_per_class.keys()):\n",
        "            for anomaly in sorted(imgpaths_per_class[classname].keys()):\n",
        "                for i, image_path in enumerate(imgpaths_per_class[classname][anomaly]):\n",
        "                    data_tuple = [classname, anomaly, image_path]\n",
        "                    if self.split == DatasetSplit.TEST and anomaly != \"good\":\n",
        "                        data_tuple.append(maskpaths_per_class[classname][anomaly][i])\n",
        "                    else:\n",
        "                        data_tuple.append(None)\n",
        "                    data_to_iterate.append(data_tuple)\n",
        "\n",
        "        return imgpaths_per_class, data_to_iterate\n"
      ],
      "metadata": {
        "id": "g7imf8-5_64x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Sequence, Union, Any, Callable\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CelebA\n",
        "import zipfile\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "from mvtec import MVTecDataset\n",
        "\n",
        "_DATASETS = {\"mvtec\": [\"patchcore.datasets.mvtec\", \"MVTecDataset\"]}\n",
        "_sub_dataset = ('bottle',  'cable',  'capsule',  'carpet',  'grid', 'hazelnut', 'leather',  'metal_nut',  'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper')\n",
        "_data_path = r'D:\\dataset\\mvtec_anomaly_detection'\n",
        "\n",
        "# Add your custom dataset class here\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, input_dataset):\n",
        "        self.dataset = input_dataset\n",
        "        Dataset()\n",
        "\n",
        "    def __len__(self):\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pass\n",
        "\n",
        "\n",
        "class MyCelebA(CelebA):\n",
        "    \"\"\"\n",
        "    A work-around to address issues with pytorch's celebA dataset class.\n",
        "\n",
        "    Download and Extract\n",
        "    URL : https://drive.google.com/file/d/1m8-EBPgi5MRubrm6iQjafK2QMHDBMSfJ/view?usp=sharing\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_integrity(self) -> bool:\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "class OxfordPets(Dataset):\n",
        "    \"\"\"\n",
        "    URL = https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 data_path_dict: dict,\n",
        "                 split: str,\n",
        "                 transform: Callable,\n",
        "                **kwargs):\n",
        "        # self.data_dir = Path(data_path) / \"OxfordPets\"\n",
        "        self.transforms = transform\n",
        "        self.imgs = []\n",
        "        for key in data_path_dict.keys():\n",
        "            imgs = data_path_dict[key][\"good\"]\n",
        "            self.imgs.extend(imgs[:int(len(imgs) * 0.75)] if split == \"train\" else imgs[int(len(imgs) * 0.75):])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = default_loader(self.imgs[idx])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, 0.0 # dummy datat to prevent breaking\n",
        "\n",
        "class VAEDataset(LightningDataModule):\n",
        "    \"\"\"\n",
        "    PyTorch Lightning data module\n",
        "\n",
        "    Args:\n",
        "        data_dir: root directory of your dataset.\n",
        "        train_batch_size: the batch size to use during training.\n",
        "        val_batch_size: the batch size to use during validation.\n",
        "        patch_size: the size of the crop to take from the original images.\n",
        "        num_workers: the number of parallel workers to create to load data\n",
        "            items (see PyTorch's Dataloader documentation for more details).\n",
        "        pin_memory: whether prepared items should be loaded into pinned memory\n",
        "            or not. This can improve performance on GPUs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str,\n",
        "        train_batch_size: int = 8,\n",
        "        val_batch_size: int = 8,\n",
        "        patch_size: Union[int, Sequence[int]] = (256, 256),\n",
        "        num_workers: int = 0,\n",
        "        pin_memory: bool = False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_dir = data_path\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.pin_memory = pin_memory\n",
        "\n",
        "    def dataset(\n",
        "    name=\"mvtec\",\n",
        "    data_path=_data_path,\n",
        "    subdatasets=_sub_dataset,\n",
        "    train_val_split=1,\n",
        "    batch_size=2,\n",
        "    resize=256,\n",
        "    imagesize=224,\n",
        "    num_workers=8,\n",
        "    augment=False,\n",
        "):\n",
        "        dataset_info = _DATASETS[name]\n",
        "        dataset_library = __import__(dataset_info[0], fromlist=[dataset_info[1]])\n",
        "\n",
        "        def get_dataloaders(seed=0):\n",
        "            dataloaders = []\n",
        "            for subdataset in subdatasets:\n",
        "                train_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                    data_path,\n",
        "                    classname=subdataset,\n",
        "                    resize=resize,\n",
        "                    train_val_split=train_val_split,\n",
        "                    imagesize=imagesize,\n",
        "                    split=dataset_library.DatasetSplit.TRAIN,\n",
        "                    seed=seed,\n",
        "                    augment=augment,\n",
        "                )\n",
        "\n",
        "                test_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                    data_path,\n",
        "                    classname=subdataset,\n",
        "                    resize=resize,\n",
        "                    imagesize=imagesize,\n",
        "                    split=dataset_library.DatasetSplit.TEST,\n",
        "                    seed=seed,\n",
        "                )\n",
        "\n",
        "                train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True,\n",
        "                )\n",
        "\n",
        "                test_dataloader = torch.utils.data.DataLoader(\n",
        "                    test_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=num_workers,\n",
        "                    pin_memory=True,\n",
        "                )\n",
        "\n",
        "                train_dataloader.name = name\n",
        "                if subdataset is not None:\n",
        "                    train_dataloader.name += \"_\" + subdataset\n",
        "\n",
        "                if train_val_split < 1:\n",
        "                    val_dataset = dataset_library.__dict__[dataset_info[1]](\n",
        "                        data_path,\n",
        "                        classname=subdataset,\n",
        "                        resize=resize,\n",
        "                        train_val_split=train_val_split,\n",
        "                        imagesize=imagesize,\n",
        "                        split=dataset_library.DatasetSplit.VAL,\n",
        "                        seed=seed,\n",
        "                    )\n",
        "\n",
        "                    val_dataloader = torch.utils.data.DataLoader(\n",
        "                        val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=num_workers,\n",
        "                        pin_memory=True,\n",
        "                    )\n",
        "                else:\n",
        "                    val_dataloader = None\n",
        "                dataloader_dict = {\n",
        "                    \"training\": train_dataloader,\n",
        "                    \"validation\": val_dataloader,\n",
        "                    \"testing\": test_dataloader,\n",
        "                }\n",
        "\n",
        "                dataloaders.append(dataloader_dict)\n",
        "            return dataloaders\n",
        "        return (\"get_dataloaders\", get_dataloaders)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "#       =========================  OxfordPets Dataset  =========================\n",
        "\n",
        "#         train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "#                                               transforms.CenterCrop(self.patch_size),\n",
        "# #                                               transforms.Resize(self.patch_size),\n",
        "#                                               transforms.ToTensor(),\n",
        "#                                                 transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "#         val_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "#                                             transforms.CenterCrop(self.patch_size),\n",
        "# #                                             transforms.Resize(self.patch_size),\n",
        "#                                             transforms.ToTensor(),\n",
        "#                                               transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "#         self.train_dataset = OxfordPets(\n",
        "#             self.data_dir,\n",
        "#             split='train',\n",
        "#             transform=train_transforms,\n",
        "#         )\n",
        "\n",
        "#         self.val_dataset = OxfordPets(\n",
        "#             self.data_dir,\n",
        "#             split='val',\n",
        "#             transform=val_transforms,\n",
        "#         )\n",
        "\n",
        "#       =========================  CelebA Dataset  =========================\n",
        "\n",
        "        train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                              transforms.CenterCrop(148),\n",
        "                                              transforms.Resize(self.patch_size),\n",
        "                                              transforms.ToTensor(),])\n",
        "\n",
        "        val_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                            transforms.CenterCrop(148),\n",
        "                                            transforms.Resize(self.patch_size),\n",
        "                                            transforms.ToTensor(),])\n",
        "\n",
        "        # self.train_dataset = MyCelebA(\n",
        "        #     self.data_dir,\n",
        "        #     split='train',\n",
        "        #     transform=train_transforms,\n",
        "        #     download=False,\n",
        "        # )\n",
        "\n",
        "        # # Replace CelebA with your dataset\n",
        "        # self.val_dataset = MyCelebA(\n",
        "        #     self.data_dir,\n",
        "        #     split='test',\n",
        "        #     transform=val_transforms,\n",
        "        #     download=False,\n",
        "        # )\n",
        "\n",
        "        _mv_dataset = MVTecDataset(source=_data_path, classname=None)\n",
        "        image_paths_per_class, data_to_iterate = _mv_dataset.get_image_data()\n",
        "        # image_paths = [x[2] for x in data_to_iterate]\n",
        "        # _, x = self.dataset(num_workers=1, train_val_split=1)\n",
        "        # _dataset = x(0)\n",
        "        self.train_dataset = OxfordPets(\n",
        "            image_paths_per_class,\n",
        "            split='train',\n",
        "            transform=train_transforms,\n",
        "        )\n",
        "\n",
        "        # Replace CelebA with your dataset\n",
        "        self.val_dataset = OxfordPets(\n",
        "            image_paths_per_class,\n",
        "            split='test',\n",
        "            transform=val_transforms,\n",
        "        )\n",
        "        # dt_training = ConcatDataset([x['training'].dataset for x in _dataset])\n",
        "        # dtloader_training = DataLoader(dt_training, batch_size=2, shuffle=True)\n",
        "\n",
        "#       ===============================================================\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.train_batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.val_batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=144,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "            pin_memory=self.pin_memory,\n",
        "        )\n",
        ""
      ],
      "metadata": {
        "id": "Cv1rl0tvAHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import argparse\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from models import *\n",
        "from experiment import VAEXperiment\n",
        "import torch.backends.cudnn as cudnn\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from dataset import VAEDataset\n",
        "from pytorch_lightning.plugins import DDPPlugin\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Generic runner for VAE models')\n",
        "parser.add_argument('--config',  '-c',\n",
        "                    dest=\"filename\",\n",
        "                    metavar='FILE',\n",
        "                    help =  'path to the config file',\n",
        "                    default='configs/vae.yaml')\n",
        "\n",
        "args = parser.parse_args()\n",
        "with open(args.filename, 'r') as file:\n",
        "    try:\n",
        "        config = yaml.safe_load(file)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "\n",
        "tb_logger =  TensorBoardLogger(save_dir=config['logging_params']['save_dir'],\n",
        "                               name=config['model_params']['name'],)\n",
        "\n",
        "# For reproducibility\n",
        "seed_everything(config['exp_params']['manual_seed'], True)\n",
        "\n",
        "model = vae_models[config['model_params']['name']](**config['model_params'])\n",
        "experiment = VAEXperiment(model,\n",
        "                          config['exp_params'])\n",
        "\n",
        "data = VAEDataset(**config[\"data_params\"], pin_memory=len(config['trainer_params']['gpus']) != 0)\n",
        "\n",
        "data.setup()\n",
        "runner = Trainer(logger=tb_logger,\n",
        "                 callbacks=[\n",
        "                     # stuff here (maybe disable)\n",
        "                     LearningRateMonitor(),\n",
        "                     ModelCheckpoint(save_top_k=2,\n",
        "                                     dirpath =os.path.join(tb_logger.log_dir , \"checkpoints\"),\n",
        "                                     monitor= \"val_loss\",\n",
        "                                     save_last= True),\n",
        "                 ],\n",
        "                #  strategy=DDPPlugin(find_unused_parameters=False),\n",
        "                 strategy=None,\n",
        "                 **config['trainer_params'])\n",
        "\n",
        "\n",
        "Path(f\"{tb_logger.log_dir}/Samples\").mkdir(exist_ok=True, parents=True)\n",
        "Path(f\"{tb_logger.log_dir}/Reconstructions\").mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "print(f\"======= Training {config['model_params']['name']} =======\")\n",
        "runner.fit(experiment, datamodule=data)"
      ],
      "metadata": {
        "id": "uTJdtpkOAMmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}