{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoa92ng/Homework/blob/main/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Hình~\\ref{fig:c2_neural_network_general} minh họa mạng Nơ-ron tổng quát, bao gồm ba tầng chính như sau: Tầng đầu vào (Input Layer), tầng ẩn (Hidden layers) và tầng đầu ra. Kích thước mạng càng sâu thì càng có nhiều lớp tầng ẩn. Mỗi hình tròn tượng trưng cho một Node, các Node này được liên kết đầy đủ tới tất cả các Node ở tầng trước với các hệ số $W$ riêng. Để tăng thêm sự linh hoạt của hàm tổng, mỗi Node sẽ có thêm một hệ số bias $b$. Như có đề cập ở phần trước, tại mỗi Node sẽ diễn ra hai quá trình tính hàm tổng và hàm kích hoạt. Cũng cần lưu ý, đối với các Node trong tầng ẩn thì đầu ra của Node ở tầng trước là đầu vào của Node ở tầng sau, dữ liệu đầu vào được coi là Layer 0 (Input Layer). Ngoài ra chỉ tồn tại các liên kết từ lớp $i$ tới lớp $i + 1$ và không tồn tại liên kết theo chiều ngược lại.\n",
        "\n",
        "Trong quá trình huấn luyện mạng Nơ-ron sẽ diễn ra hai quá trình lan truyền thẳng (Feedforward) và lan truyền ngược (Backpropagation). Trong quá trình lan truyền thằng giá trí của mỗi Node sẽ được cập nhật thông qua giá trị đầu vào $X$, bộ trọng số $W$, hàm tổng $Z$ và hàm kích hoạt $\\sigma$. Còn đối với quá trình lan truyền ngược bộ giá trị trọng số $W$ sẽ được cập nhật thông qua các thuật toán như Gradient Descent và đạo hàm chuỗi.\n",
        "\\begin{enumerate}[\\bfseries a)]\n",
        "\t\\item {Hoạt động của mạng truyền thẳng}\\par\n",
        "\tMạng truyền thẳng nhiều lớp (Feed forward neural network) là mạng có nhiều lớp ngoài lớp đầu vào và lớp đầu ra. Nó hoạt động theo nguyên tắc kết quả đầu ra của lớp $i$ sẽ là đầu vào của lớp $i + 1$ (trừ lớp đầu ra). Mỗi một lớp có bộ trọng số và vectơ Bias riêng. Các bước cơ bản của quá trình lan truyền thẳng như sau: Tính hàm tổng (\\eqref{eq:sum_z}), tính hàm kích hoạt (\\eqref{eq:activation_a}).\n",
        "\t\\begin{equation*}\\footnote{https://machinelearningcoban.com/2017/02/24/mlp/}\n",
        "\t\t\\begin{aligned}\n",
        "\t\t\t\\textbf{$W$}^(l) \\in \\mathbb{R}^{d^(l-1)xd^(l)}\n",
        "\t\t\t\\textbf{$b$}^(l) \\in \\mathbb{R}^{d^(l)x1}\n",
        "\t\t\tz_j^(l) = w_j^{(l)T}a^(l-1) + b_j^(l)\n",
        "\t\t\t\\textbf{$z$}^(l) =\\textbf{$W$}^(l)T\\textbf{$a$}^(l-1) + \\textbf{$b$}(l)\n",
        "\t\t\\end{aligned}\n",
        "\t\t\\label{eq:sum_z}\n",
        "\t\t\\begin{aligned_2}\n",
        "\t\t\t\\textbf{$a$^}^(l) = \\sigma(\\textbf{$z$}^(l)\n",
        "\t\t\\end{aligned_2}\n",
        "\t\t\\label{eq:activation_a}\n",
        "\t\\end{equation*}\n",
        "\\end{enumerate}\n",
        "\\begin{enumerate}[\\bfseries b)]\n",
        "\t\\item {Quá trình lan truyền ngược (Backpropagation)}\\par\n",
        "\tNăm 2012, tại cuộc thi ILSVRC,\n",
        "\\end{enumerate}\n",
        "\\begin{figure}[h!]\n",
        "\t\\begin{center}\n",
        "\t\t\\includegraphics[width=0.65\\linewidth]{images/c2/multi_layers.png}\n",
        "\t\t\\caption{Mạng Nơ-ron tổng quát.\\protect\\footnotemark}\n",
        "\t\t\\label{fig:c2_neural_network_general}\n",
        "\t\\end{center}\n",
        "\\end{figure}\n",
        "\\footnotetext{https://machinelearningcoban.com/2017/02/24/mlp/}\n",
        "\n",
        "\\subsection{Mạng tích chập (Convolutional Neural Network)}"
      ],
      "metadata": {
        "id": "JHU48OiK3eKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}