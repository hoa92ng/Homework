{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoa92ng/Homework/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1aWSFb-ZR8TTIDdRlDBBCh-YvvCxmt6Bc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvy7R559bvjl",
        "outputId": "7d38b4c6-d67f-4066-88d1-8879974149e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1ppSSIuo0o740Z-Wfz6m47xQtmHd08znO bitsandbytes-0.43.0.dev0-cp311-cp311-win_amd64.whl\n",
            "Processing file 1I6I0pdL4QNcozYaWng7i1GnmPmqwWJZC deepspeed-0.13.1+unknown-py3-none-any.whl\n",
            "Processing file 1xU2jfkljiVuqqQjCkYPVa1KUFR8QF_SB llvm-5e5a22ca-windows-x64.tar.gz\n",
            "Processing file 1Xt_XGgc_wgl1f079oSjDNfCBhd7iXnzC triton-2.1.0-cp311-cp311-win_amd64.whl\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ppSSIuo0o740Z-Wfz6m47xQtmHd08znO\n",
            "To: /content/311Wheels/bitsandbytes-0.43.0.dev0-cp311-cp311-win_amd64.whl\n",
            "100% 24.1M/24.1M [00:00<00:00, 60.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I6I0pdL4QNcozYaWng7i1GnmPmqwWJZC\n",
            "To: /content/311Wheels/deepspeed-0.13.1+unknown-py3-none-any.whl\n",
            "100% 961k/961k [00:00<00:00, 129MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1xU2jfkljiVuqqQjCkYPVa1KUFR8QF_SB\n",
            "From (redirected): https://drive.google.com/uc?id=1xU2jfkljiVuqqQjCkYPVa1KUFR8QF_SB&confirm=t&uuid=a9ba3ff5-8a1b-4cdd-8d66-ffac721d5d99\n",
            "To: /content/311Wheels/llvm-5e5a22ca-windows-x64.tar.gz\n",
            "100% 1.33G/1.33G [00:13<00:00, 97.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Xt_XGgc_wgl1f079oSjDNfCBhd7iXnzC\n",
            "From (redirected): https://drive.google.com/uc?id=1Xt_XGgc_wgl1f079oSjDNfCBhd7iXnzC&confirm=t&uuid=64c976aa-79e9-4657-b217-03a5be8448e8\n",
            "To: /content/311Wheels/triton-2.1.0-cp311-cp311-win_amd64.whl\n",
            "100% 760M/760M [00:12<00:00, 62.3MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/311Wheels/llvm-5e5a22ca-windows-x64.tar.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ESxbFSPNhNLb",
        "outputId": "4ca5fb27-2761-4198-b6ce-a301f42918f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40d9cabd-9814-48a7-bc65-e4b60713adb2\", \"llvm-5e5a22ca-windows-x64.tar.gz\", 1328893580)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "import random\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "from transformers import set_seed as transformers_set_seed\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = r\"\\llama3\\Llama-3-8b-Instruct\"\n",
        "\n",
        "HAS_BFLOAT16 = torch.cuda.is_bf16_supported()\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "source_train_file_ko_en = \"ko_en.txt\"\n",
        "target_train_file_ko_en = \"en_ko.txt\"\n",
        "source_train_file_vi_en = \"vi_en.txt\"\n",
        "target_train_file_vi_en = \"en_vi.txt\"\n",
        "source_train_file_vi_ko = \"vi_ko.txt\"\n",
        "target_train_file_vi_ko = \"ko_vi.txt\"\n",
        "\n",
        "with open(source_train_file_ko_en, encoding=\"utf-8\") as source0, open(target_train_file_ko_en, encoding=\"utf-8\") as target0:\n",
        "  source_sentences0 = [sent0.strip() for sent0 in source0.readlines()]\n",
        "  target_sentences0 = [sent0.strip() for sent0 in target0.readlines()]\n",
        "\n",
        "with open(source_train_file_vi_en, encoding=\"utf-8\") as source1, open(target_train_file_vi_en, encoding=\"utf-8\") as target1:\n",
        "  source_sentences1 = [sent1.strip() for sent1 in source1.readlines()]\n",
        "  target_sentences1 = [sent1.strip() for sent1 in target1.readlines()]\n",
        "\n",
        "with open(source_train_file_vi_ko, encoding=\"utf-8\") as source2, open(target_train_file_vi_ko, encoding=\"utf-8\") as target2:\n",
        "  source_sentences2 = [sent2.strip() for sent2 in source2.readlines()]\n",
        "  target_sentences2 = [sent2.strip() for sent2 in target2.readlines()]\n",
        "\n",
        "def create_prompt():\n",
        "    prompts = []\n",
        "    #for i in range(0, len(source_sentences)):\n",
        "    for i in range(0, 100):\n",
        "      prompt_koen = \"[INST]\"+\" Translate the Korean input text into English: \" + source_sentences0[i] + \" [/INST] \" + target_sentences0[i] + \" </s>\"\n",
        "      prompt_vien = \"[INST]\"+\" Translate the Vietnamese input text into English: \" + source_sentences1[i] + \" [/INST] \" + target_sentences1[i] + \" </s>\"\n",
        "      prompt_enko = \"[INST]\"+\" Translate the English input text into Korean: \" + target_sentences0[i] + \" [/INST] \" + source_sentences0[i] + \" </s>\"\n",
        "      prompt_envi = \"[INST]\"+\" Translate the English input text into Vietnamese: \" + target_sentences1[i] + \" [/INST] \" + source_sentences1[i] + \" </s>\"\n",
        "\n",
        "      print(prompt_vien)\n",
        "      prompts.append(prompt_koen)\n",
        "      prompts.append(prompt_vien)\n",
        "      prompts.append(prompt_enko)\n",
        "      prompts.append(prompt_envi)\n",
        "    for i in range(0,len(source_train_file_vi_ko)):\n",
        "      prompt_kovi = \"[INST]\"+\" Translate the Korean input text into Vietnamese: \" + target_sentences2[i] + \" [/INST] \" + source_sentences2[i] + \" </s>\"\n",
        "      prompt_viko = \"[INST]\"+\" Translate the Vietnamese input text into Korean: \" + source_sentences2[i] + \" [/INST] \" + target_sentences2[i] + \" </s>\"\n",
        "      prompts.append(prompt_kovi)\n",
        "      prompts.append(prompt_viko)\n",
        "    return prompts\n",
        "\n",
        "prompts = create_prompt()\n",
        "random.shuffle(prompts)\n",
        "\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\"text\": prompts[:len(prompts)]}),\n",
        "    \"validation\": Dataset.from_dict({\"text\": prompts[int(0.9*len(prompts)):]})\n",
        "})\n",
        "\n",
        "\n",
        "random_state = 3407\n",
        "transformers_set_seed(random_state)\n",
        "\n",
        "output_directory = \"llama3_20240711\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "                                  output_dir = output_directory,\n",
        "                                  num_train_epochs=2,\n",
        "                                  #max_steps = 594, # comment out this line if you want to train in epochs\n",
        "                                  per_device_train_batch_size = 2,\n",
        "                                  gradient_accumulation_steps = 1,\n",
        "                                  optim=\"paged_adamw_8bit\",\n",
        "                                  fp16 = not HAS_BFLOAT16,\n",
        "                                  bf16 = HAS_BFLOAT16,\n",
        "                                  save_steps=1000,\n",
        "                                  logging_steps=50,\n",
        "                                  # save_strategy=\"epoch\",\n",
        "                                  evaluation_strategy=None,\n",
        "                                  learning_rate=2e-4,\n",
        "                                  weight_decay=0.001,\n",
        "                                  max_grad_norm=0.3,\n",
        "                                  max_steps=-1,\n",
        "                                  warmup_ratio=0.03,\n",
        "                                  group_by_length=True,\n",
        "                                  lr_scheduler_type=\"linear\",\n",
        "                                  report_to=None,\n",
        "                                  seed=random_state\n",
        "                                )\n",
        "\n",
        "max_seq_length = 1024  # increase if needed\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "                    model=model,\n",
        "                    # peft_config=lora_config,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    tokenizer=tokenizer,\n",
        "                    packing=False,\n",
        "                    dataset_text_field=\"text\",\n",
        "                    args=training_args,\n",
        "                    train_dataset=dataset[\"train\"],\n",
        "                    eval_dataset=dataset[\"validation\"],\n",
        "                  )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "shtVSCttk_tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/sdsldkfj...somefileid.. -O /some_parent_directory/some_child_directory"
      ],
      "metadata": {
        "id": "AB4G_jHzflRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/drive/folders/1aWSFb-ZR8TTIDdRlDBBCh-YvvCxmt6Bc'\n",
        "output = '20150428_collected_images.tgz'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "Dyu-imLXdAeX",
        "outputId": "9cf0fb52-d0b7-448a-db9c-9dd96950b871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/drive/folders/1aWSFb-ZR8TTIDdRlDBBCh-YvvCxmt6Bc\n",
            "To: /content/20150428_collected_images.tgz\n",
            "1.21MB [00:00, 65.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20150428_collected_images.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install pyarrow"
      ],
      "metadata": {
        "id": "MI5HTcBrm0Gt",
        "outputId": "061532f1-d398-41e4-856d-58caba44104d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "370e4396adb94ae09ac301d028278a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset('werty1248/EnKo-Translation-LongTextOnly-dedup', split='train')"
      ],
      "metadata": {
        "id": "23ACSspfm_WA",
        "outputId": "a64da428-a974-4623-8fc0-385bab78febe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "2049d7d3c1a34b67a0d2d786e7131447",
            "4e4177e5028c4cdd80f0c9c67d4abfb9",
            "1ee8f623bcb24f869d61b5f59f9ec5da",
            "9a1136dea92b423482dcae23a0c0ab93",
            "0dfd9e1270cc466b8e5373ad96b8ac71",
            "61150267ee904349b8e2b7a03275769a",
            "ca83701bd74849c39a6123e8993c3342",
            "a8d07425a34c46b199fd11856628c715",
            "9d6931aabb914ae3817d3f5ea0064c83",
            "7221e49efcfb4d4a94d9f911eabb021a",
            "cdde8ee1662b4f6cb929ea5d0c7306c5",
            "5b11600d7af946519c24cead9f77e0b6",
            "178b75ec06df455f8d7e96c00a8f18e6",
            "243fddec87ea479184422384cbae0014",
            "5f6e975dce024ea4946c86c2d4d726f2",
            "8b16377544334c6d98488c039b060aaa",
            "b42cdece2b074eaa8439426673b02d35",
            "c6e909ea174e43e0876bd8e712dca8da",
            "c1cf84fc20664f7ab2fe2162d442862b",
            "95c969a07f534e07b995b7434238d988",
            "7fb3759f3a0d48109d7b927773494b3a",
            "e4e1831ea7ad4b559dd7a54cfa894f26",
            "9818ac10469649f1b817aba8b3f06c74",
            "771f28545bf8403da9cbc72331a456ba",
            "f74661dab1d545f4bf9890a07680a45c",
            "f46d6d7a45da421788e6d5e3d410413e",
            "dd806d7e8db944fbbf864dc2949f9bad",
            "c3d960ff9c0d4e3fb55fcfc813257e52",
            "0d82dd97bd0843b4ac5686bf8e94443f",
            "56d91e0252974d50b1b762d8511246e7",
            "214e94e13dfa4f0fba28a29ef3654c75",
            "41af48a009a44e3c9f16f08f8dde11e4",
            "2c373cc9ddc94bf1afd77246b36d87ac",
            "f0099804eae04723bd4489c7167bd121",
            "044335d26ccd4a1fa49cb2e4f356b542",
            "3bbc877d66c048c6981d9404eaa3776d",
            "730f03112fe84828ba66420e415f66dd",
            "5bbf699c47ca445d925f646ba552b117",
            "8fcff9f28676448e909dd6c7af0d1468",
            "dc48dd6144b944cdabacad7f9ec1e621",
            "3f333071512e4848b3ecb91dac8f832f",
            "46e174a9be004de0a7f96cf7361d1af9",
            "fd5fa094d21f453285bad6e6aa67bd3d",
            "b501b36db45948518ab7197ef597077b",
            "aa0ec882a0f24db1a83243f2d754687d",
            "41f1b24c375c42638f0d41796dd4ccf8",
            "8b07650313094dd9860283f62f96606b",
            "7b62e36377284050bd1063c1787b9e43",
            "592ddecb7e4040beb4dd6f58e77b2f76",
            "20ccc2fce46d40608752cc5be465f339",
            "d7b32e26ea944ccb804040c11c840bd7",
            "4f82b89368f349c6a85153b120a2481c",
            "c6ec3844d7e74574991524481dd966bd",
            "ba5c7f30ee184599998159bab06aa977",
            "359770fc195f4047bcf927709d7b594c"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2049d7d3c1a34b67a0d2d786e7131447"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/317M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b11600d7af946519c24cead9f77e0b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/54.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9818ac10469649f1b817aba8b3f06c74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0099804eae04723bd4489c7167bd121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/161009 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa0ec882a0f24db1a83243f2d754687d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset.to_pandas()"
      ],
      "metadata": {
        "id": "2MJ9VfZynK9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.head().iterrows():\n",
        "  print(row['english'], '\\n')"
      ],
      "metadata": {
        "id": "C8XCQTbdnOqF",
        "outputId": "88bce56d-8b1a-42e6-e72b-0317686fd90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROOFTOP GREENING STRUCTURETo provide a structure firmly and easily installing a house cultivation arch-like aggregate in a rooftop greening structure. This rooftop greening structure includes pressingly fixing each of support stands 24 in each of support stand line groups 24A to a rooftop slab surface through a greening support layer 6, using a fastener which pierces into the greening support layer 6, and steps over between each of the support stands 24 and the rooftop slab surface 2, and installing a holding member 36 for holding a house cultivation arch-like aggregate 50 each on the upper end surface of each of the support stands 24 in each of the support stand line groups 24A. As a result of this, the support stand 24 which has stiffness higher than the greening support layer 6, and is firmly fixed to the rooftop slab surface 2 through the greening support layer 6 is used for holding the end part of the arch-like aggregate 50. The holding member 36 for holding the end part of the arch-like aggregate 50 is installed on the upper end surface of the support stand 24 so as to suppress the holding member 36 from burying in soil and increase the visibility.In a rooftop greening structure in which a greening support layer is formed by laying a plurality of greening support panels on the rooftop floor and soil is arranged on the greening support layer, a pair of support stands are placed on the greening support layer. The rows are arranged so as to be separated from each other, and each of the support rows is arranged upright so as to form a row with a plurality of supports having higher rigidity than the greening support layer. Each support pedestal in each support pedestal row group is configured through the greening support layer by using a fastener that penetrates the greening support layer and straddles between each support pedestal and the rooftop floor surface. It is characterized in that it is pressed and fixed to the rooftop floor surface, and the upper end surface of each support stand in each support stand row group is provided with a holding portion for holding an arch-shaped aggregate for house cultivation. Rooftop greening structure. \n",
            "\n",
            "Native chicken breeding methodThe invention discloses a native chicken breeding method, which includes steps that the shield degree of a breeding grove is 60-65%; a native chicken activity field with area of 5-8 mu is encircled bya 1.8-2.2m high nylon mesh; a ventilating and warming device is arranged in a henhouse; feed and water are delivered at 8: 00-15: 00 in every morning, and native chicken are put in grove activity field at 15:00-17: 00 in the afternoon; music is displayed at 17: 00-18: 30, and feed is delivered at outside of the henhouse to domesticize the native chickens , and then chickens are returned to the henhouse; the henhouse is cleaned at intervals of 12-15 days; the henhouse is sterilized by an automatic sterilizing system during the stocking period in the afternoon at intervals of 3-5 days. The native chicken breeding method can well consider about the stocking time, thus the stocking instinct of the native chickens is well guaranteed, the food intake of the native chickens is increased throughthe reasonable captive time; the meat growth is accelerated, the breeding cycle is shortened, and the meat quality of the native chickens is ensured.A kind of 1. cultural method of chicken, it is characterised in that：it the described method comprises the following steps：（1）selection cultivation ground：selection away from livestock and poultry transaction place, slaughtering field, chemical plant, garbage disposal plant, avoid air, dust, water source, germ and the cultivation of the woods of noise pollution, the moon degree of covering of the woods is 60~65%, with 1.8~2.2 meters of high nylon net circle area is 5~8 mu of chicken playground, and vegetable seeds is broadcasted sowing in forest land；（2）build chicken house：the wind sheltering in woods ground on the sunny side, hen house is built in the chicken playground centre position that physical features is high and dry, draining blowdown condition is good, and ventilation heating is set in hen house equipment, hen house is interior to set automatic sterilizing system；（3）select kind：select it is resistance to it is extensive, action flexibly, the pure native that power of looking for food is strong, premunition is strong；（4）dietary management：every mu of forest land puts 260~280 in a suitable place to breed, every morning 8:00~15:feed and water are launched in stable breeding when 00, afternoon 15:00~17:it is put into forest land playground when 00 to put in a suitable place to breed, 17:00~18:dispensing feed outside music colony house is played when 30 to enter row domestication makes chicken return to colony house, and day temperature is maintained at 20~23 degrees celsius in circle, and nocturnal temperature is maintained at 20~23 degrees celsius； （5）disinfectant management：to being cleaned in hen house, colony house is started certainly during chicken is put in a suitable place to breed afternoon within every 3~5 days within every 12~15 days dynamic disinfection system is sterilized, and lime powder for every 2~3 months to the main passageway in woods forest land. \n",
            "\n",
            "REINFORCING STRUCTURE AND CONSTRUCTION METHOD FOR GREENING VEGETATION ARTICLESA reinforcing structure and a construction method for greening vegetation articles are provided. The reinforcing structure for greening vegetation articles comprises a vegetation article formed on a surface of an area to be greened, a locating cover (2), a locating component (3) and a rope (4). The locating component is pressed into the area to be greened. A rotating supporting pivot (31) is arranged on the locating component. One end of the rope is fixedly connected to the rotating supporting pivot, and the other end is exposed out of the area to be greened and passes through the vegetation article to reach the locating cover. The locating cover fixedly clamps the other end of the rope and presses the vegetation article. The rope can pull the locating component to rotate when the locating component is pressed into the area to be greened, which ensures that the locating component generates a large tension resistance in the area to be greened and can not be pulled out of the area to be greened easily. At the same time the locating cover is further pulled to tightly press on the vegetation article by the rope, so that the vegetation article can be firmly located on the surface of the area to be greened. The structure has good fixing effect and can effectively replace traditional anchoring rod.A structure for reinforcing a greening member, comprising: a greening member adapted for being placed on a surface of an afforesting area; a positioning cover adapted for pressing on the greening member; a positioning member provided with a pivot portion, which is adapted for being inserted into the ground of the afforesting area; and a rope, one end of which is fastened to the pivot portion, and the other end of which is extended out from the ground of the afforesting area and extended through the greening member to the positioning cover, where the other end of the rope is fixed by the positioning cover, so that the rope is tensioned between the positioning cover and the positioning member, with the positioning cover pressing on the greening member. \n",
            "\n",
            "CARPOPHORE GROWING GUIDE BODY FOR MUSHROOM CULTIVATION, AND METHOD FOR DRYING CARPOPHORE GROWING GUIDE BODY FOR MUSHROOM CULTIVATIONTo greatly reduce the drying time by preventing close adhesion of adjacent carphophore growing guide bodies for mushroom cultivation to each other in the vertical direction in the drying process when carpophore growing guide bodies are reused after harvesting mushrooms. A carpophore growing guide body 10 is formed in the shape of an inverted truncated cylinder by a sheet body 12, and placed on a shoulder of a mouth of a mushroom cultivation bottle to limit the shape of growing of a carpophore by enclosing the upper part of the mouth of the bottle. The outer peripheray of the carpophore growing guide body 10 for mushroom cultivation has a tongue piece 20 locked to the upper edge 10A of a cylindrical mouth of a lower carpophore growing guide body 10 for mushroom cultivation so that a space 30 is formed between the upper and lower carpophore growing guide bodies 10 for mushroom cultivation when the upper carpophore growing guide body 10 for mushroom cultivation is stacked on the lower carpophore growing guide body 10 for mushroom cultivation in such a way that the lower part of the upper carpophore growing guide body is put into the cylindrical mouth of the lower carpophore growing guide body.A guide for growing fruiting bodies for mushroom cultivation, which is formed into an inverted cone-shaped cylinder by a sheet body, and is placed on the shoulder of the bottle mouth of a mushroom cultivation bottle to surround the upper part of the bottle mouth to regulate the growth shape of fruiting bodies. In the body, when the lower part of the upper mushroom cultivation fruiting body growth guide body is stacked so as to enter the tube mouth of the lower mushroom cultivation fruiting body growth guide body, the upper and lower mushroom cultivation fruiting body growth guide bodies A feature is that a tongue piece that is locked to the upper edge of the tube mouth of the lower fruiting body growth guide for mushroom cultivation is provided on the outer periphery of the fruiting body growth guide for mushroom cultivation so as to create a gap between them. A fruiting body growth guide for mushroom cultivation. \n",
            "\n",
            "AUTOMATIC PLANTING APPARATUSAn object of the present invention is to save labor by automating an operation for planting plant support blocks in holes in a hydroponic panel. As a means for achieving this object of the present invention, an automatic planting apparatus (10) is provided which is adapted to slice, by a block separating unit (25), l pieces of blocks (4) starting with a first row of blocks on a plant support sheet (3) on an auxiliary conveyor (13) at an n-1 pieces' interval, plant these blocks (4) by a block planting unit (39) in l pieces of holes (2) in a first row of holes in a panel (1) on a primary conveyor (11), feed the panel (1) forward by a distance corresponding to the pitch of the holes by a primary feeder (15) while slidingly moving the panel (1) laterally via the auxiliary conveyor (13) by a distance corresponding to the width of one block (4) by a sliding unit 17, and repeat similar block planting operations.an automatic planting apparatus consists of a main conveyer to convey a panel in which holes are arranged along ℓ number of lines and a plural number of rows, a sub-conveyer to convey a plant supporting sheet devided into blocks along m number of lines and a plural number of rows by cutting lines and arrranged above or below said main conveyer so as to slide right and left, a block separator arranged near the front side of said sub-conveyer and tearing off ℓ pieces of blocks in the same row from said plant supporting sheet, and a block inserting unit inserting ℓ pieces of blocks torn off by said block separator into ℓ number of holes in the same row respectively, wherein said main conveyer has a main feeder to feed said panel forward by a pitch between rows when all blocks have been inserted into ℓ number of holes in a row, and said sub-conveyer has a sliding unit to slide said plant supporting sheet sideways by a distance corresponding to the width of each block when all blocks have been inserted into holes arranged in a row of said panel and a sub-feeder feeding said plant supporting sheet forward by a distance corresponding to the length of each block when all blocks in the front row have been torn off, and m is an integral number times of ℓ . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import sys, os\n",
        "# print(os.getcwd())\n",
        "sys.path.append('./src')\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import patchcore\n",
        "from patchcore import patchcore as patchcore_model\n",
        "from siamese import SiameseNetwork\n",
        "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from typing import Callable, Any\n",
        "import glob, os\n",
        "import random\n",
        "\n",
        "class Custom_Dataset_All(Dataset):\n",
        "    \"\"\"\n",
        "    URL = https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 image_folder: str = r\"D:\\Non_Documents\\9.Document\\Dataset\\mvtech_anomaly_detection\\mvtech_anomaly_detection\",\n",
        "                 image_name: str = \"bottle\",\n",
        "                 split: str = \"train\",\n",
        "                 transform: Callable = None,\n",
        "                **kwargs):\n",
        "        self._sub_dataset = ('bottle',  'cable',  'capsule',  'carpet',  'grid', 'hazelnut', 'leather',  'metal_nut',  'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper')\n",
        "        self.image_name = image_name\n",
        "        self.img_folder = image_folder\n",
        "        self.transforms = transform\n",
        "        self.imgs_pos, self.imgs_neg = self.get_image_paths(img_folder=image_folder)\n",
        "\n",
        "    def get_image_paths(self, img_folder, split: str = \"train\"):\n",
        "        img_positive = []\n",
        "        img_negative = []\n",
        "        # Define the file extensions for images\n",
        "        image_extensions = ('*.png', '*.jpg', '*.jpeg', '*.gif', '*.bmp', '*.tiff')\n",
        "        # Initialize an empty list to store image paths\n",
        "        for img_name in self._sub_dataset:\n",
        "            for ext in image_extensions:\n",
        "                img_fold = os.path.join(img_folder, img_name, \"train\", \"good\")\n",
        "                if (img_name == self.image_name):\n",
        "                    img_positive.extend(glob.glob(os.path.join(img_fold, ext)))\n",
        "                else:\n",
        "                    img_negative.extend(glob.glob(os.path.join(img_fold, ext)))\n",
        "\n",
        "        img_positive = img_positive[:int(len(img_positive) * 0.75)] if split == \"train\" else img_positive[int(len(img_positive) * 0.75):]\n",
        "        img_positive = [self.pil_loader(x) for x in img_positive]\n",
        "        img_negative = [self.pil_loader(x) for x in img_negative]\n",
        "        random.shuffle(img_positive)\n",
        "        random.shuffle(img_negative)\n",
        "        return img_positive, img_negative\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_pos)\n",
        "\n",
        "    def pil_loader(self, path: str) -> Image.Image:\n",
        "        # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "        with open(path, \"rb\") as f:\n",
        "            img = Image.open(f).convert(\"RGB\")\n",
        "\n",
        "            if self.transforms is not None:\n",
        "                img = self.transforms(img)\n",
        "            return img\n",
        "\n",
        "    def default_loader(self, path: str) -> Any:\n",
        "            return self.pil_loader(path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        valid_indices = [i for i in range(len(self.imgs_pos)) if i != idx]\n",
        "        random_index = random.choice(valid_indices)\n",
        "        img_anchor = self.imgs_pos[idx]\n",
        "        img_pos = self.imgs_pos[random_index]\n",
        "        img_neg = self.imgs_neg[random.randint(0, len(self.imgs_neg) - 1)]\n",
        "        return img_anchor, img_pos,  img_neg\n",
        "\n",
        "def _standard_patchcore(image_dimension):\n",
        "    patchcore_instance = patchcore_model.PatchCore(torch.device(\"cpu\"))\n",
        "    backbone = models.wide_resnet50_2(pretrained=False)\n",
        "    backbone.name, backbone.seed = \"wideresnet50\", 0\n",
        "    patchcore_instance.load(\n",
        "        backbone=backbone,\n",
        "        layers_to_extract_from=[\"layer2\", \"layer3\"],\n",
        "        device=torch.device(\"cpu\"),\n",
        "        input_shape=[3, image_dimension, image_dimension],\n",
        "        pretrain_embed_dimension=1024,\n",
        "        target_embed_dimension=1024,\n",
        "        patchsize=3,\n",
        "        patchstride=1,\n",
        "        spade_nn=2,\n",
        "        sub_model=None\n",
        "    )\n",
        "    return patchcore_instance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    epoch_num = 20\n",
        "    batch_size = 4\n",
        "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "    network = SiameseNetwork(in_dimension=3).to(device='cuda')\n",
        "    criterion = nn.TripletMarginLoss()\n",
        "    criterion_2 = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
        "    transfomation = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "\n",
        "        ])\n",
        "    _dataset = Custom_Dataset_All(transform=transfomation)\n",
        "    _data_loader = DataLoader(_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    _dataset_test = Custom_Dataset_All(transform=transfomation, split=\"test\")\n",
        "    _test_data_loader = DataLoader(_dataset_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    mean, std = 0, 0.2\n",
        "    # model = _standard_patchcore(224)\n",
        "    gaussian_noise = torch.randn(size=(batch_size, 3, 224, 224), device='cuda') * std + mean\n",
        "    for epoch in range(epoch_num):\n",
        "        print(f\"=========== epoch {epoch} ===========\")\n",
        "        loss_item = 0.\n",
        "        valid_loss = 0.\n",
        "        network.train()\n",
        "        for i, image in enumerate(_data_loader):\n",
        "            if (image[0].shape[0] < batch_size): continue\n",
        "            # image = torch.concat(image, dim=0)\n",
        "            # feature = model.get_feature(image)\n",
        "            bs, c, h, w = image[0].shape\n",
        "            img0, img1, img2 = image[0], image[1], image[0].cuda() + gaussian_noise\n",
        "            img0, img1, img2 = img0.cuda(), img1.cuda(), img2\n",
        "            img2.requires_grad = True\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output1, output2, output3 = network(img0, img1, img2)\n",
        "            loss = criterion(output1, output2, output3) + criterion_2(img0, img2)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                xs = img2.grad\n",
        "                gaussian_noise -= 0.001 * img2.grad\n",
        "            loss_item += loss.item()\n",
        "            if i % 1 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epoch_num}], Step [{i}/{len(_data_loader)}], Loss: {(loss_item/(i+1)):.4f}\")\n",
        "\n",
        "\n",
        "        network.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, image in enumerate(_test_data_loader):\n",
        "                if (image[0].shape[0] < 2): continue\n",
        "                img0, img1, img2 = image[0], image[1], image[0].cuda() + gaussian_noise\n",
        "                img0, img1, img2 = img0.cuda(), img1.cuda(), img2\n",
        "\n",
        "                output1, output2, output3 = network(img0, img1, img2)\n",
        "                loss = criterion(output1, output2, output3) + criterion_2(img0, img2)\n",
        "                valid_loss += loss.item()\n",
        "            print(f\"Epoch [{epoch+1}/{epoch_num}], Valid Loss: {(valid_loss/(len(_data_loader)+1)):.4f}\")\n",
        "\n",
        "\n",
        "    torch.save(network.state_dict(), f'./sub_model/siamese_wide_resnet50_2_final.pth')"
      ],
      "metadata": {
        "id": "mqBlFPyDntIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
        "from custom_dataset import AudioDataset\n",
        "from transformers import AutoFeatureExtractor\n",
        "from datasets import Dataset, DatasetDict\n",
        "from atc_audio import Audio_Implement\n",
        "import numpy as np\n",
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"file\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16_000*2, truncation=True\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = './model_1'\n",
        "    # model_path = './model_tiny/New folder'\n",
        "\n",
        "    # model_path = './my_trained_model_tiny_cls/checkpoint-1782'\n",
        "    dataset = AudioDataset(root_dir=\"./data\", class_dir=\"./data/Voice_Class.txt\")\n",
        "\n",
        "\n",
        "    model = AutoModelForAudioClassification.from_pretrained(\n",
        "        model_path, num_labels=len(dataset.class_bank), label2id=dataset.class_bank, id2label=dataset.class_bank_reverse, ignore_mismatched_sizes=True)\n",
        "\n",
        "    print(\"class number: {}\".format(len(dataset.class_bank)))\n",
        "    print(dataset.class_bank)\n",
        "\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)\n",
        "\n",
        "    data_train = Dataset.from_dict(dataset.dataset_train_source)\n",
        "    Audio_Implement.find_all_homophones()\n",
        "    dataset_train = data_train.cast_column(\"file\", Audio_Implement(sampling_rate=16_000))\n",
        "\n",
        "    data_test = Dataset.from_dict(dataset.dataset_test_source)\n",
        "    dataset_test = data_test.cast_column(\"file\", Audio_Implement(sampling_rate=16_000))\n",
        "\n",
        "    dataset_dict = DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
        "\n",
        "    encoded_minds = dataset_dict.map(preprocess_function, remove_columns=\"file\", batched=True)\n",
        "    # encoded_minds = encoded_minds.rename_column(\"intent_class\", \"label\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"my_trained_model_tiny_cls_29_c\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=4,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=50,\n",
        "        warmup_ratio=0.1,\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        push_to_hub=False,\n",
        "        report_to='none',\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=encoded_minds[\"train\"].with_format('torch'),\n",
        "        eval_dataset=encoded_minds[\"test\"].with_format('torch'),\n",
        "        tokenizer=feature_extractor,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "Tj_3kY1_5g5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
        "from custom_dataset import AudioDataset\n",
        "from transformers import AutoFeatureExtractor\n",
        "from datasets import Dataset, DatasetDict, Audio\n",
        "from network import Wave_Network\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# class Wave_Model(nn.Module):\n",
        "#     def __init__(self, *args, **kwargs) -> None:\n",
        "#         super().__init__(*args, **kwargs)\n",
        "def collate_fn(batch):\n",
        "    # Sử dụng pad_sequence để padding tất cả các chuỗi đến chiều dài lớn nhất trong batch\n",
        "    input_ids = [torch.Tensor(item['input_values']) for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    # for i in range(len(batch)):\n",
        "    #     batch[i]['input_values'] = input_ids[i]\n",
        "    # batch = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    return {\n",
        "        'input_values': input_ids,\n",
        "        'label': torch.tensor(labels)\n",
        "    }\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"file\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16_000*2, truncation=True,\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # model_path = './model_1'\n",
        "    model_path = './model_1'\n",
        "    epoch_num = 100\n",
        "    batch_size = 32\n",
        "    dataset = AudioDataset(root_dir=\"./data\", class_dir=\"./data/Voice_Class_anomaly.txt\")\n",
        "\n",
        "    model = Wave_Network(num_classes=len(dataset.class_bank), model_path=model_path).to(device='cuda')\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_path)\n",
        "\n",
        "    data_train = Dataset.from_dict(dataset.dataset_train_source)\n",
        "    dataset_train = data_train.cast_column(\"file\", Audio(sampling_rate=16_000))\n",
        "\n",
        "    data_test = Dataset.from_dict(dataset.dataset_test_source)\n",
        "    dataset_test = data_test.cast_column(\"file\", Audio(sampling_rate=16_000))\n",
        "    dataset_dict = DatasetDict({\"train\": dataset_train, \"test\": dataset_test})\n",
        "\n",
        "    data_dict = dataset_dict.map(preprocess_function, remove_columns=\"file\", batched=True)\n",
        "\n",
        "    train_dataloader = DataLoader(data_dict['train'].with_format('torch'), batch_size, collate_fn=collate_fn)\n",
        "    test_dataloader = DataLoader(data_dict['test'].with_format('torch'), batch_size, collate_fn=collate_fn)\n",
        "\n",
        "    criterion_1 = nn.CrossEntropyLoss()\n",
        "    criterion_2 = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.get_train_params(), lr=0.001)\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epoch_num):\n",
        "        loss_item = 0.\n",
        "        valid_loss = 0.\n",
        "        running_loss = 0\n",
        "        save_loss = 999999\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, labels = data['input_values'].cuda(), data['label'].cuda()\n",
        "            true_project_label = torch.ones(size=(inputs.shape[0], 1))\n",
        "            false_project_label = torch.zeros(size=(inputs.shape[0], 1))\n",
        "            projection_labels = torch.concat((true_project_label, false_project_label)).cuda()\n",
        "            o_nomaly, o_classification = model(inputs)\n",
        "            loss = criterion_1(o_classification, labels) + criterion_2(o_nomaly, projection_labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:  # Print every 10 batches\n",
        "                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {loss.item()}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(test_dataloader):\n",
        "                inputs, labels = data['input_values'].cuda(), data['label'].cuda()\n",
        "                true_project_label = torch.ones(size=(inputs.shape[0], 1))\n",
        "                false_project_label = torch.zeros(size=(inputs.shape[0], 1))\n",
        "                projection_labels = torch.concat((true_project_label, false_project_label)).cuda()\n",
        "                o_nomaly, o_classification = model(inputs)\n",
        "                loss = criterion_1(o_classification, labels) + criterion_2(o_nomaly, projection_labels)\n",
        "                valid_loss += loss.item()\n",
        "            valid_loss = valid_loss/len(test_dataloader)\n",
        "            if epoch == 0: valid_loss = save_loss\n",
        "            else:\n",
        "                if valid_loss < save_loss:\n",
        "                    torch.save(model.state_dict(), f'./w2vec/models/w2vec_best.pth')\n",
        "                    save_loss = valid_loss\n",
        "            print(f\"Epoch [{epoch+1}/{epoch_num}], Valid Loss: {valid_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f'./w2vec/models/w2vec_final.pth')\n"
      ],
      "metadata": {
        "id": "wqdHQc_H5iL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import Wav2Vec2Model, TrainingArguments, Trainer\n",
        "from custom_dataset import AudioDataset\n",
        "from transformers import AutoFeatureExtractor\n",
        "from datasets import Dataset, DatasetDict\n",
        "import torch\n",
        "from torch import functional as F\n",
        "\n",
        "class projection_MLP(nn.Module):\n",
        "    def __init__(self, in_dim=256, hidden_dim=256, out_dim=256):\n",
        "        super().__init__()\n",
        "        ''' page 3 baseline setting\n",
        "        Projection MLP. The projection MLP (in f) has BN ap-\n",
        "        plied to each fully-connected (fc) layer, including its out-\n",
        "        put fc. Its output fc has no ReLU. The hidden fc is 2048-d.\n",
        "        This MLP has 3 layers.\n",
        "        '''\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            # nn.BatchNorm1d(out_dim)\n",
        "        )\n",
        "        self.num_layers = 3\n",
        "    def set_layers(self, num_layers):\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.num_layers == 3:\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "        elif self.num_layers == 2:\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer3(x)\n",
        "        else:\n",
        "            raise Exception\n",
        "        return x\n",
        "\n",
        "class Wave_Network(nn.Module):\n",
        "    def __init__(self, num_classes=18, model_path='', device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.backbone = Wav2Vec2Model.from_pretrained(model_path).to(device=device)\n",
        "        # self.backbone = nn.Sequential(*(list(self.backbone.children()))[:-2])\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.linear = nn.Linear(in_features=768, out_features=256)\n",
        "        # self.projection = projection_MLP().to(device=device)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.LazyLinear(256),\n",
        "            nn.LazyLinear(256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.tail = nn.Sequential(\n",
        "                projection_MLP().to(device=device),\n",
        "                nn.BatchNorm1d(256),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.LazyLinear(1, bias=False)\n",
        "        )\n",
        "        self.classifier = nn.LazyLinear(num_classes)\n",
        "\n",
        "    def get_train_params(self):\n",
        "        return [\n",
        "            {'params': self.projection.parameters()},\n",
        "            {'params': self.linear.parameters()},\n",
        "            {'params': self.tail.parameters()},\n",
        "            {'params': self.classifier.parameters()},\n",
        "        ]\n",
        "    def forward(self, x, std=0.05, is_train=True):\n",
        "        with torch.no_grad():\n",
        "            x = self.backbone(x).last_hidden_state\n",
        "        x = self.linear(x)\n",
        "        x_hidden_state = torch.mean(x, dim=1)\n",
        "        # x = torch.max(x, dim=1).values\n",
        "        # x = self.backbone(x).projector\n",
        "        x_classification = self.classifier(x_hidden_state)\n",
        "\n",
        "        x_projector = self.projection(x_hidden_state)\n",
        "        # add noise\n",
        "        if is_train:\n",
        "            noise = torch.normal(mean=0, std=std, size=x_projector.shape).to(device=self.device)\n",
        "            x_noise = x_projector + noise\n",
        "            x_noise = torch.concat((x_projector, x_noise))\n",
        "        else:\n",
        "            x_noise = x_projector\n",
        "        x_anomaly = self.tail(x_noise)\n",
        "\n",
        "        return x_anomaly, x_classification\n",
        "\n",
        "\n",
        "\n",
        "# model = Wave_Network(model_path='./model_1')\n",
        "# print(model)\n"
      ],
      "metadata": {
        "id": "oluamppV5kIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "from transformers import pipeline\n",
        "from custom_dataset import AudioDataset\n",
        "from datasets import Dataset, Audio, DatasetDict\n",
        "from network import Wave_Network\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_path = './model_1'\n",
        "checkpoint_path = './W2Vec/models/w2vec_best.pth'\n",
        "dataset = AudioDataset(root_dir=r\"D:xxg\\Voice\\Label\\labeling\", class_dir=\"./data/Voice_Class_anomaly.txt\", is_test=True)\n",
        "model = Wave_Network(num_classes=len(dataset.class_bank), model_path=model_path, device='cpu')\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "model = model.to(device='cpu')\n",
        "model.eval()\n",
        "data_train = Dataset.from_dict(dataset.dataset_test_source)\n",
        "data_train = data_train.cast_column(\"file\", Audio(sampling_rate=16_000))\n",
        "\n",
        "count = 0\n",
        "for i in range(len(data_train)):\n",
        "    print(f'\\npredicted result {data_train[i][\"raw_path\"]}:')\n",
        "    audio_file = data_train[i][\"file\"]['array']\n",
        "    input_tensor = torch.from_numpy(audio_file).unsqueeze(0).to(dtype=torch.float32, device='cpu')\n",
        "    # padding = (0, 32000 - input_tensor.shape[-1])\n",
        "    # input_tensor = F.pad(input_tensor, padding, \"constant\", 0)\n",
        "    with torch.no_grad():\n",
        "        o_nomaly, o_classification = model(input_tensor, is_train=False)\n",
        "        print(torch.sigmoid(o_nomaly)[0].item(), dataset.data_bank[torch.argmax(o_classification, dim=-1)[0].item()])"
      ],
      "metadata": {
        "id": "D65fvGq55l5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from datasets import Audio\n",
        "import torch\n",
        "import torchaudio\n",
        "from typing import Dict, Optional, Union\n",
        "import numpy as np\n",
        "import random\n",
        "import os, sys\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "sys.path.append('./MeloTTS')\n",
        "from MeloTTS.melo.api import TTS\n",
        "tts_model = TTS(language=\"EN\")\n",
        "\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import cmudict\n",
        "nltk.download('cmudict')\n",
        "d = cmudict.dict()\n",
        "\n",
        "import pronouncing\n",
        "import random\n",
        "import librosa\n",
        "\n",
        "\n",
        "# List of words to avoid\n",
        "avoid_phrases = ['Stand up', 'Sit down', 'Lie down', 'Go ahead', 'Move forward',\n",
        "                            'Go straight', 'Go forward', 'Go backward', 'Reverse', 'Move backward',\n",
        "                            'Go back', 'Stop', 'Go backward', 'Turn left', 'Turn right',\n",
        "                            'Greeting', 'Love', 'Twist to the left', 'Twist to the right', 'Shake hand',\n",
        "                            'Jump forward', 'Climbing mode', 'Stretch', 'Hi Lucy']\n",
        "speaker_ids = tts_model.hps.data.spk2id\n",
        "speakers = list(speaker_ids.keys())\n",
        "# Get a list of all meaningful English words\n",
        "english_words = set(words.words())\n",
        "homophones_commands = []\n",
        "\n",
        "class Audio_Implement(Audio):\n",
        "\n",
        "    # def __init__(self, sampling_rate):\n",
        "    #     super().__init__(sampling_rate=sampling_rate)\n",
        "\n",
        "    def find_homophones(self, word_list):\n",
        "        \"\"\"Trả về danh sách các từ phát âm gần giống nhưng nghĩa khác\"\"\"\n",
        "        homophones = {}\n",
        "        for word in word_list:\n",
        "            pronunciation = d.get(word.lower())\n",
        "            if pronunciation:\n",
        "                key = tuple(pronunciation[0])\n",
        "                if key in homophones:\n",
        "                    homophones[key].append(word)\n",
        "                else:\n",
        "                    homophones[key] = [word]\n",
        "        return [words for words in homophones.values() if len(words) > 1]\n",
        "\n",
        "    @staticmethod\n",
        "    def find_all_homophones():\n",
        "        for command in avoid_phrases:\n",
        "            homophones = Audio_Implement.find_homophones(command)\n",
        "            if homophones:\n",
        "                random.shuffle(homophones)\n",
        "                if len(homophones) > 100:\n",
        "                    homophones_commands.extend(homophones[:100])\n",
        "                else:\n",
        "                    homophones_commands.extend(homophones)\n",
        "        random.shuffle(homophones_commands)\n",
        "    @staticmethod\n",
        "    def find_homophones(command):\n",
        "\n",
        "        words = command.split()\n",
        "        homophone_sentences = []\n",
        "\n",
        "        # Duyệt qua từng từ trong câu lệnh\n",
        "        for i, word in enumerate(words):\n",
        "            # Tìm tất cả các từ có phát âm giống với từ hiện tại\n",
        "            pronunciations = pronouncing.phones_for_word(word)\n",
        "            homophones = set()\n",
        "\n",
        "            for pronunciation in pronunciations:\n",
        "                homophones.update(pronouncing.search(pronunciation))\n",
        "\n",
        "            homophones.discard(word)  # Bỏ từ gốc ra khỏi danh sách từ đồng âm\n",
        "            for homophone in homophones:\n",
        "                # Tạo câu lệnh mới với từ đồng âm\n",
        "                new_sentence = words[:i] + [homophone] + words[i+1:]\n",
        "                homophone_sentences.append(\" \".join(new_sentence))\n",
        "        return homophone_sentences\n",
        "\n",
        "    # Function to generate random meaningful words that are not in the avoid list\n",
        "    def generate_unique_meaningful_words(self, max_num_words=3):\n",
        "        meaningful_words = list(english_words)\n",
        "        # Filter out words that are part of the avoid phrases\n",
        "        filtered_words = [word for word in meaningful_words if not any(phrase.lower().startswith(word) for phrase in avoid_phrases)]\n",
        "        num_words = random.randint(1, max_num_words)\n",
        "        gen = ' '.join(random.sample(filtered_words, num_words))\n",
        "        # print(\"gend text: {}\".format(gen))\n",
        "        return gen\n",
        "\n",
        "    def decode_example(self, value: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None) -> dict:\n",
        "        snr_db = 10\n",
        "        if value[\"path\"] == \"None\":\n",
        "            sample_rate = 16000  # 16 kHz\n",
        "            duration_seconds = 2  # 5 seconds\n",
        "            num_samples = sample_rate * duration_seconds\n",
        "            random_audio_data = np.random.randn(num_samples)\n",
        "            print(\"-----------------------------silence-----------------------------\")\n",
        "            return {\"path\": \"None\", \"array\": random_audio_data, \"sampling_rate\": 16000}\n",
        "\n",
        "        elif value[\"path\"] == \"Unknown\":\n",
        "            words = self.generate_unique_meaningful_words(3)\n",
        "            print(words)\n",
        "            random_audio_data = tts_model.tts_to_file(words, speaker_ids[speakers[random.randint(0, len(speaker_ids) - 1)]], None, speed=1., noise_scale=0.1, noise_scale_w=0.2)\n",
        "            random_audio_data = librosa.to_mono(random_audio_data)\n",
        "            random_audio_data = librosa.resample(random_audio_data, orig_sr=44100, target_sr=16000)\n",
        "\n",
        "            if random.randint(0, 1) == 1:\n",
        "                noise = np.random.normal(0, 1, random_audio_data.shape[0])\n",
        "                signal_power = np.mean(random_audio_data**2)\n",
        "                noise_power = np.mean(noise**2)\n",
        "                scaling_factor = np.sqrt(signal_power / (10**(snr_db / 10) * noise_power))\n",
        "                scaled_noise = noise * scaling_factor\n",
        "                random_audio_data = random_audio_data + scaled_noise\n",
        "            return {\"path\": \"unknown\", \"array\": random_audio_data, \"sampling_rate\": 16000}\n",
        "\n",
        "        else:\n",
        "            data = super().decode_example(value, token_per_repo_id)\n",
        "            if random.randint(0, 1) == 1:\n",
        "                # noise = np.random.rand(data[\"array\"].shape[0]) * 0.002\n",
        "                noise = np.random.normal(0, 1, data[\"array\"].shape[0])\n",
        "                signal_power = np.mean(data[\"array\"]**2)\n",
        "                noise_power = np.mean(noise**2)\n",
        "                scaling_factor = np.sqrt(signal_power / (10**(snr_db / 10) * noise_power))\n",
        "                scaled_noise = noise * scaling_factor\n",
        "                data[\"array\"] = data[\"array\"] + scaled_noise\n",
        "            return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class Audio_Implement(Audio):\n",
        "#     def decode_example(self, value: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None) -> dict:\n",
        "#         if value[\"path\"] != \"None\":\n",
        "#             data = super().decode_example(value, token_per_repo_id)\n",
        "#             if random.randint(0, 1) == 1:\n",
        "#                 noise = np.random.rand(data[\"array\"].shape[0]) * 0.002\n",
        "#                 data[\"array\"] = data[\"array\"] + noise\n",
        "#             return data\n",
        "#         else:\n",
        "#             sample_rate = 16000  # 16 kHz\n",
        "#             duration_seconds = 2  # 5 seconds\n",
        "#             num_samples = sample_rate * duration_seconds\n",
        "#             # random_audio_data = torch.randn(num_samples)\n",
        "#             random_audio_data = np.random.rand(num_samples)\n",
        "#             # random_audio_data = random_audio_data / torch.max(torch.abs(random_audio_data))\n",
        "#             # audio_tensor = random_audio_data.unsqueeze(0)\n",
        "#             # audio_bytes = torchaudio.functional.mu_law_encoding(audio_tensor, quantization_channels=256).squeeze().byte().numpy()\n",
        "#             return {\"path\": \"None\", \"array\": random_audio_data, \"sampling_rate\": 16000}\n"
      ],
      "metadata": {
        "id": "RCRk0-Jb5uS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoFeatureExtractor\n",
        "from datasets import load_dataset, Audio\n",
        "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "dict_label = {'yes':0,\n",
        "              'no':1,\n",
        "              'up':2,\n",
        "              'down':3,\n",
        "              'left':4,\n",
        "              'right':5,\n",
        "              'on':6,\n",
        "              'off':7,\n",
        "              'stop':8,\n",
        "              'go':9,\n",
        "              'unknown':10,\n",
        "              'silence':11}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(labels, y_pred)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # You can return additional metrics if needed\n",
        "    accuracy = accuracy_score(labels, y_pred)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "#     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    inputs = feature_extractor(\n",
        "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16_000, truncation=True\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def edit_label(examples):\n",
        "    # valid_dataset.features['label'].names = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', '_silence_']\n",
        "    for i, x in enumerate(examples['file']):\n",
        "        a = examples['label'][i]\n",
        "        if examples['is_unknown'][i]: examples['label'][i] = dict_label['unknown']\n",
        "        elif id2label[str(examples['label'][i])] == '_silence_':\n",
        "            examples['label'][i] = dict_label['silence']\n",
        "    return examples\n",
        "\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained('./model_1')\n",
        "train_dataset_ = load_dataset(\"google/speech_commands\", 'v0.01', split='train', trust_remote_code=True)\n",
        "valid_dataset_ = load_dataset(\"google/speech_commands\", 'v0.01', split='validation', trust_remote_code=True)\n",
        "test_dataset_ = load_dataset(\"google/speech_commands\", 'v0.01', split='test', trust_remote_code=True)\n",
        "\n",
        "labels = valid_dataset_.features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "\n",
        "train_dataset_ = train_dataset_.map(edit_label, batched=True)\n",
        "valid_dataset_ = valid_dataset_.map(edit_label, batched=True)\n",
        "test_dataset_ = test_dataset_.map(edit_label, batched=True)\n",
        "\n",
        "df = train_dataset_.to_pandas()\n",
        "# Giả sử cột label trong dataset có tên là 'label'\n",
        "label_counts = df['label'].value_counts()\n",
        "\n",
        "# Hiển thị kết quả\n",
        "print(label_counts)\n",
        "\n",
        "# # Hiển thị đồ thị\n",
        "# label_counts.plot(kind='bar')\n",
        "# plt.xlabel('Class')\n",
        "# plt.ylabel('Số lượng')\n",
        "# plt.title('Số lượng mỗi class trong cột label')\n",
        "# plt.show()\n",
        "\n",
        "train_dataset_ = train_dataset_.remove_columns([\"file\", \"is_unknown\", \"speaker_id\", \"utterance_id\"])\n",
        "valid_dataset_ = valid_dataset_.remove_columns([\"file\", \"is_unknown\", \"speaker_id\", \"utterance_id\"])\n",
        "test_dataset_ = test_dataset_.remove_columns([\"file\", \"is_unknown\", \"speaker_id\", \"utterance_id\"])\n",
        "\n",
        "labels = dict_label.keys()\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "print(label2id)\n",
        "\n",
        "\n",
        "\n",
        "train_data = train_dataset_.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "valid_data = valid_dataset_.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "test_dataset = test_dataset_.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "\n",
        "encoded_data_train = train_data.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
        "encoded_data_validation = valid_data.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
        "encoded_test_validation = test_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
        "print(encoded_data_train)\n",
        "\n",
        "num_labels = len(id2label)\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    # \"./model_1\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
        "    \"./my_awesome_mind_model/checkpoint-3990\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
        ")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_mind_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_data_train.with_format(\"torch\"),\n",
        "    eval_dataset=encoded_data_validation.with_format(\"torch\"),\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "# trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=encoded_test_validation.with_format(\"torch\"))\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "id": "qZl-WNo9VsHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ctranslate2\n",
        "import transformers\n",
        "\n",
        "\n",
        "# model = ctranslate2.Translator(r\"D:\\1.Project\\6.ChatBot\\convert\\convert_m2m-9548\", device=\"cuda\", device_index=[0]) # file convert ra dc\n",
        "model = ctranslate2.Translator(r\"xxxconvert\\convert_m2m\") # file convert ra dc\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(r\"xxx\\convert\\checkpoint-9548\") # copy các file token ở model gốc ra\n",
        "\n",
        "ct2-transformers-converter --model D:\\1.Project\\6.ChatBot\\convert\\checkpoint-23870 --output_dir D:\\1.Project\\6.ChatBot\\convert\\convert_m2m --force\n",
        "\n",
        "src_lang = 'ko'\n",
        "tgt_lang = 'vi'\n",
        "tokenizer.src_lang = src_lang\n",
        "batch_sentences = [\n",
        "    \"Quỹ đầu tư Hermes đã sử dụng dữ liệu từ khu vực châu Âu và có được những dự đoán riêng của mình để tính toán chính xác các nước đang phải đối mặt những khó khăn lớn nhất.\",\n",
        "]\n",
        "# batch_tokens = [tokenizer.encode(sentence, )[0].tolist() for sentence in batch_sentences]\n",
        "target_prefix = [[tokenizer.lang_code_to_token[tgt_lang]]] * len(batch_sentences)\n",
        "source_sentences = [[tokenizer.lang_code_to_token[src_lang]] + tokenizer.tokenize(sentence) for sentence in batch_sentences]\n",
        "input_tokens = []\n",
        "for i in range(0, len(batch_sentences)):\n",
        "    input_tokens.append(tokenizer.convert_ids_to_tokens(tokenizer.encode(batch_sentences[i])))\n",
        "translations = model.translate_batch(input_tokens, target_prefix=target_prefix)\n",
        "translations = [translation[0]['tokens'][1:] for translation in translations]\n",
        "translations = [tokenizer.convert_tokens_to_string(translation) for translation in translations]\n",
        "for translation in translations:\n",
        "    print(translation)"
      ],
      "metadata": {
        "id": "YMmHEUPBXZFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2049d7d3c1a34b67a0d2d786e7131447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e4177e5028c4cdd80f0c9c67d4abfb9",
              "IPY_MODEL_1ee8f623bcb24f869d61b5f59f9ec5da",
              "IPY_MODEL_9a1136dea92b423482dcae23a0c0ab93"
            ],
            "layout": "IPY_MODEL_0dfd9e1270cc466b8e5373ad96b8ac71"
          }
        },
        "4e4177e5028c4cdd80f0c9c67d4abfb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61150267ee904349b8e2b7a03275769a",
            "placeholder": "​",
            "style": "IPY_MODEL_ca83701bd74849c39a6123e8993c3342",
            "value": "Downloading readme: 100%"
          }
        },
        "1ee8f623bcb24f869d61b5f59f9ec5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8d07425a34c46b199fd11856628c715",
            "max": 1928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d6931aabb914ae3817d3f5ea0064c83",
            "value": 1928
          }
        },
        "9a1136dea92b423482dcae23a0c0ab93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7221e49efcfb4d4a94d9f911eabb021a",
            "placeholder": "​",
            "style": "IPY_MODEL_cdde8ee1662b4f6cb929ea5d0c7306c5",
            "value": " 1.93k/1.93k [00:00&lt;00:00, 86.2kB/s]"
          }
        },
        "0dfd9e1270cc466b8e5373ad96b8ac71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61150267ee904349b8e2b7a03275769a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca83701bd74849c39a6123e8993c3342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8d07425a34c46b199fd11856628c715": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d6931aabb914ae3817d3f5ea0064c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7221e49efcfb4d4a94d9f911eabb021a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdde8ee1662b4f6cb929ea5d0c7306c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b11600d7af946519c24cead9f77e0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_178b75ec06df455f8d7e96c00a8f18e6",
              "IPY_MODEL_243fddec87ea479184422384cbae0014",
              "IPY_MODEL_5f6e975dce024ea4946c86c2d4d726f2"
            ],
            "layout": "IPY_MODEL_8b16377544334c6d98488c039b060aaa"
          }
        },
        "178b75ec06df455f8d7e96c00a8f18e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b42cdece2b074eaa8439426673b02d35",
            "placeholder": "​",
            "style": "IPY_MODEL_c6e909ea174e43e0876bd8e712dca8da",
            "value": "Downloading data: 100%"
          }
        },
        "243fddec87ea479184422384cbae0014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1cf84fc20664f7ab2fe2162d442862b",
            "max": 317225489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95c969a07f534e07b995b7434238d988",
            "value": 317225489
          }
        },
        "5f6e975dce024ea4946c86c2d4d726f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fb3759f3a0d48109d7b927773494b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_e4e1831ea7ad4b559dd7a54cfa894f26",
            "value": " 317M/317M [00:06&lt;00:00, 52.0MB/s]"
          }
        },
        "8b16377544334c6d98488c039b060aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42cdece2b074eaa8439426673b02d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e909ea174e43e0876bd8e712dca8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1cf84fc20664f7ab2fe2162d442862b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95c969a07f534e07b995b7434238d988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fb3759f3a0d48109d7b927773494b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e1831ea7ad4b559dd7a54cfa894f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9818ac10469649f1b817aba8b3f06c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_771f28545bf8403da9cbc72331a456ba",
              "IPY_MODEL_f74661dab1d545f4bf9890a07680a45c",
              "IPY_MODEL_f46d6d7a45da421788e6d5e3d410413e"
            ],
            "layout": "IPY_MODEL_dd806d7e8db944fbbf864dc2949f9bad"
          }
        },
        "771f28545bf8403da9cbc72331a456ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3d960ff9c0d4e3fb55fcfc813257e52",
            "placeholder": "​",
            "style": "IPY_MODEL_0d82dd97bd0843b4ac5686bf8e94443f",
            "value": "Downloading data: 100%"
          }
        },
        "f74661dab1d545f4bf9890a07680a45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d91e0252974d50b1b762d8511246e7",
            "max": 54520173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_214e94e13dfa4f0fba28a29ef3654c75",
            "value": 54520173
          }
        },
        "f46d6d7a45da421788e6d5e3d410413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41af48a009a44e3c9f16f08f8dde11e4",
            "placeholder": "​",
            "style": "IPY_MODEL_2c373cc9ddc94bf1afd77246b36d87ac",
            "value": " 54.5M/54.5M [00:01&lt;00:00, 48.9MB/s]"
          }
        },
        "dd806d7e8db944fbbf864dc2949f9bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d960ff9c0d4e3fb55fcfc813257e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d82dd97bd0843b4ac5686bf8e94443f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56d91e0252974d50b1b762d8511246e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214e94e13dfa4f0fba28a29ef3654c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41af48a009a44e3c9f16f08f8dde11e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c373cc9ddc94bf1afd77246b36d87ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0099804eae04723bd4489c7167bd121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_044335d26ccd4a1fa49cb2e4f356b542",
              "IPY_MODEL_3bbc877d66c048c6981d9404eaa3776d",
              "IPY_MODEL_730f03112fe84828ba66420e415f66dd"
            ],
            "layout": "IPY_MODEL_5bbf699c47ca445d925f646ba552b117"
          }
        },
        "044335d26ccd4a1fa49cb2e4f356b542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fcff9f28676448e909dd6c7af0d1468",
            "placeholder": "​",
            "style": "IPY_MODEL_dc48dd6144b944cdabacad7f9ec1e621",
            "value": "Downloading data: 100%"
          }
        },
        "3bbc877d66c048c6981d9404eaa3776d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f333071512e4848b3ecb91dac8f832f",
            "max": 11216240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46e174a9be004de0a7f96cf7361d1af9",
            "value": 11216240
          }
        },
        "730f03112fe84828ba66420e415f66dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd5fa094d21f453285bad6e6aa67bd3d",
            "placeholder": "​",
            "style": "IPY_MODEL_b501b36db45948518ab7197ef597077b",
            "value": " 11.2M/11.2M [00:00&lt;00:00, 28.1MB/s]"
          }
        },
        "5bbf699c47ca445d925f646ba552b117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcff9f28676448e909dd6c7af0d1468": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc48dd6144b944cdabacad7f9ec1e621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f333071512e4848b3ecb91dac8f832f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e174a9be004de0a7f96cf7361d1af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd5fa094d21f453285bad6e6aa67bd3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b501b36db45948518ab7197ef597077b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa0ec882a0f24db1a83243f2d754687d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41f1b24c375c42638f0d41796dd4ccf8",
              "IPY_MODEL_8b07650313094dd9860283f62f96606b",
              "IPY_MODEL_7b62e36377284050bd1063c1787b9e43"
            ],
            "layout": "IPY_MODEL_592ddecb7e4040beb4dd6f58e77b2f76"
          }
        },
        "41f1b24c375c42638f0d41796dd4ccf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20ccc2fce46d40608752cc5be465f339",
            "placeholder": "​",
            "style": "IPY_MODEL_d7b32e26ea944ccb804040c11c840bd7",
            "value": "Generating train split: 100%"
          }
        },
        "8b07650313094dd9860283f62f96606b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f82b89368f349c6a85153b120a2481c",
            "max": 161009,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6ec3844d7e74574991524481dd966bd",
            "value": 161009
          }
        },
        "7b62e36377284050bd1063c1787b9e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5c7f30ee184599998159bab06aa977",
            "placeholder": "​",
            "style": "IPY_MODEL_359770fc195f4047bcf927709d7b594c",
            "value": " 161009/161009 [00:14&lt;00:00, 11455.68 examples/s]"
          }
        },
        "592ddecb7e4040beb4dd6f58e77b2f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20ccc2fce46d40608752cc5be465f339": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b32e26ea944ccb804040c11c840bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f82b89368f349c6a85153b120a2481c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6ec3844d7e74574991524481dd966bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba5c7f30ee184599998159bab06aa977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359770fc195f4047bcf927709d7b594c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}